{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0853fa4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CRC Workshop: Machine Learning with Functional Connectivity (FC) Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdac596",
   "metadata": {},
   "source": [
    "## The AOMIC Data\n",
    "\n",
    "First, lets load the data and inspect it a bit. The [AOMIC dataset](https://nilab-uva.github.io/AOMIC.github.io/) is a collection data obtained in three different studies (**PIOP1**, **PIOP2**, **ID1000**). Here, we will be only concerned with the data from the **ID1000** study, which aimed to collect 1000 fMRI scans during movie-watching. The next cell defines the path to all the **ID1000** specific data, and also adds the names of the two files we will be interested in. One of these files contains the preprocessed **functional connectivity (FC)** data, whereas the other file contains the important **demographic** information. Let's start by also loading the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e368676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to ID1000 data within the AOMIC datalad dataset\n",
    "ID1000_path = (\n",
    "    Path(\"..\") / \"aomic-fc\"/ \"junifer_storage\" / \n",
    "    \"JUNIFER_AOMIC_TSV_CONNECTOMES\" / \"ID1000\"\n",
    ")\n",
    "\n",
    "# Path to the demographics data file\n",
    "demographics_path = ID1000_path / \"ID1000_participants.tsv\"\n",
    "\n",
    "# Path to the connectomes data file\n",
    "# The name of this file is a bit of a mouthful but contains important\n",
    "# information\n",
    "connectomes_path = ID1000_path / (\n",
    "    \"ID1000_BOLD_parccortical-Schaefer100x17FSLMNI_\"\n",
    "    \"parcsubcortical-TianxS2x3TxMNInonlinear2009cAsym_\"\n",
    "    \"marker-empiricalFC_moviewatching.tsv.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd60e7",
   "metadata": {},
   "source": [
    "### Demographic Data\n",
    "\n",
    "Now that we have defined these paths let's load each file and look at them one by one. Let's start with the demographics. We will load it using pandas, and as you might see from the file extension, these are both **tsv** files and we will therefore load them using a tab as a delimiter. In addition, we will load the first column as the index of the dataframe as it happens to contain the subject ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f086b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographics = pd.read_csv(demographics_path, sep=\"\\t\", index_col=0)\n",
    "demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6b297",
   "metadata": {},
   "source": [
    "We can see some of the standard demographic variables, like \"age\", \"sex\", \"BMI\", and so on. As you might be able to tell, however, this file not *only* contains \"demographic\" information but also some other participant data, as for example cognitive measurements (e.g. \"IST_memory\", \"IST_fluid\").\n",
    "\n",
    "### Connectomes\n",
    "\n",
    "Let us now check the connectomes out to see for which subjects we have preprocessed functional connectivity data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e404919",
   "metadata": {},
   "outputs": [],
   "source": [
    "connectomes = pd.read_csv(connectomes_path, sep=\"\\t\", index_col=0, compression=\"gzip\")\n",
    "connectomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f5a83",
   "metadata": {},
   "source": [
    "In this dataframe again, **each row** corresponds to *one subject* from the study. **Each column** represents a *unique pairwise relationship* between two brain areas (also called an *edge* in graph theory terminology). That is, since a brain parcellation with **N** areas results in an **NxN** symmetric correlation matrix per subject, one half of a subjects matrix is discarded. Similarly, the diagonal of this correlation matrix is also typically discarded as the correlation of an area with itself is always 1. The remaining entries can be stacked and result in one row of this dataframe. Thus, each row contains **N x (N-1) / 2** entries. In our case, since the connectomes were processed with a combination of the Schaefer 100 cortical parcellation and the Tian 32 subcortical parcellation, this results in **100 x (100 - 1) / 2 = 8646** columns. This concept is illustrated by the graphic below:\n",
    "\n",
    "![title](images/connectomes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69a1e6",
   "metadata": {},
   "source": [
    "### Subsetting the data\n",
    "\n",
    "As you can see, the first dataframe on demographics contains 928 rows (i.e. subjects), whereas the second dataframe contains 877 rows. Let us for further analyses only select subjects for which we actually have connectomes. But first, let's also make sure, that we identify any 'NaN' values in the functional connectivity data and remove subjects with any 'NaN' entries.\n",
    "\n",
    "The pandas **isna()** method will check for each entry in the dataframe whether it is 'NaN' or not. That is, if an entry is 'NaN' it will return True and otherwise it will return False. We can use this to identify the indices (i.e. subjects) for which there are 'NaN' entries by combining it with the **any()** method provided by pandas.\n",
    "\n",
    "First see the output from **isna()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e0553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "isna = connectomes.isna()\n",
    "isna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437ec81",
   "metadata": {},
   "source": [
    "The **any()** method will return whether any element along a given axis (i.e. along a row or a column) is True. The following output should be \"True\" therefore, if a subject has 'NaN' values and \"False\" otherwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de12920",
   "metadata": {},
   "outputs": [],
   "source": [
    "isna_any = isna.any(axis=1)\n",
    "isna_any"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71d656b",
   "metadata": {},
   "source": [
    "We can do calculations on these boolean values as if they are 0's and 1's. That is, \"True\" will be counted as 1 and \"False\" will be counted as 0. We can therefore use the **.sum()** method to determine the number of 'NaN' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5686c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "isna_any.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fd78c",
   "metadata": {},
   "source": [
    "The output (\"0\") shows us that there aren't any 'NaN' values, so we can simply proceed with the data we have here. Let us therefore now subset the demographic data for which we have connectomes. That is, we will index the demographics dataset using the index from the connectomes dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490e76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_demographics = demographics.loc[connectomes.index]\n",
    "subsampled_demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d92e5",
   "metadata": {},
   "source": [
    "The indexing using the **.loc()** method importantly also ensures that the rows in both dataframes are in the same order which will be important later when we convert them to numpy arrays, a data structure that scikit-learn understands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed3214",
   "metadata": {},
   "source": [
    "### Exploring our sample:\n",
    "\n",
    "Now that the samples in the connectome data and the demographics data are matched, let's take a quick look at sex and age to get an overview of our sample.\n",
    "\n",
    "The **value_counts()** method takes a pandas series (i.e. a column from the dataframe) and counts the amount of times each possible value is contained in the column. This is a good way of discovering what values are possible for a specific variable, and how many instances there are for each value. This is useful for example when looking at categorial variables, for example \"sex\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee755abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_demographics[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413de870",
   "metadata": {},
   "source": [
    "The **plot.hist()** pandas method provides a quick way of making a histogram that we can also group by \"sex\" to look at each distribution seperately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e340d0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "subsampled_demographics.plot.hist(column=[\"age\"], by=\"sex\", figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa930a",
   "metadata": {},
   "source": [
    "As you can see, the age range is quite narrow, and limited to young people. This is a common problem in neuroimaging or psychology studies, which often sample students from their universities for convenience. It is always good to be aware of these limitations before starting any complicated machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce09c14",
   "metadata": {},
   "source": [
    "## Doing some ML\n",
    "\n",
    "Now, lets try to build a classifier that can distinguish between males and females given a functional connectome. That is, the connectomes will be the features ('X') and the sex will be the target ('y'). Since \"sex\" in our data is encoded as \"male\" and \"female\" and scikit-learn only understands numeric data, we have to convert \"sex\" to a numeric, categorial variable. Since it's a binary target, this is relatively straightforward. We can do this simply by adding another column to our demographics data as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86ccfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_demographics[\"sex_numeric\"] = subsampled_demographics[\"sex\"].map(lambda x: 1 if x == \"female\" else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dff35c",
   "metadata": {},
   "source": [
    "We can inspect the output as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458a222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_info = subsampled_demographics[[\"sex\", \"sex_numeric\"]]\n",
    "sex_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4cd7d8",
   "metadata": {},
   "source": [
    "As you can see, there is a 1 where sex is given as \"female\" and a 0 where sex is given as male. Let's finalise the target as a numpy array which is a data structure scikit-learn understands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c50c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y = np.array(sex_info[\"sex_numeric\"])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913d282",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "Let us first split the data so we have one hold-out validation set that will be left untouched for now. Let us also finalise our features as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a127d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(connectomes)\n",
    "\n",
    "X_model_selection, X_holdout, y_model_selection, y_holdout = train_test_split(X, y, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea919d4b",
   "metadata": {},
   "source": [
    "We will then do another train-test split on the model selection data, so that we can train models on the inner training set, compare their performance on the inner test set, and then evaluate our final model on the hold-out validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e505a997",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_inner_train, X_inner_test, y_inner_train, y_inner_test = \\\n",
    "    train_test_split(X_model_selection, y_model_selection, random_state=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b3ef4",
   "metadata": {},
   "source": [
    "### Fitting a bunch of models\n",
    "\n",
    "Every problem, every classification or regression task is different, and therefore requires a different model. That is, which model works best depends on the underlying processes that generate the distribution of our X and on the \"true\" function that maps X to y. In other words, we cannot really know which model will work best before we try it out. Let us test out a few popular options therefore starting with a **Ridge Classifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd61e3f",
   "metadata": {},
   "source": [
    "We can initialise the RidgeClassifier object, and then fit it on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2388fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed43a8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.fit(X_inner_train, y_inner_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bb990",
   "metadata": {},
   "source": [
    "We can now test the accuracy of this classifier by making predictions on our test set that was so far unseen. The predictions can then be compared to the true values of y in this test set using some metric (for example \"accuracy\" in our case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge = ridge.predict(X_inner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd8131",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae474b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inner_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df5b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_inner_test, predictions_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd44a9",
   "metadata": {},
   "source": [
    "However, if we check out the **sklearn documentation**, we see that the **RidgeClassifier** has a parameter called **alpha**, which can be set to a positive floating point value. This is a **hyperparameter**, *that must be set by the user and cannot be fitted based on the data*. How can we know which value is the best one? First, run the code in the next cell to see the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7078beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "?RidgeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f69e4f",
   "metadata": {},
   "source": [
    "A simple approach could be to define a grid of potential candidate values, repeat the fitting and testing procedure for each of them, and then simply select the one that yields the highest accuracy. One might do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5371aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "alpha_candidates = [0.001, 0.01, 0.1, 1, 2, 10, 50, 100]\n",
    "for alpha in alpha_candidates:\n",
    "    ridge = RidgeClassifier(alpha=alpha)\n",
    "    ridge.fit(X_inner_train, y_inner_train)\n",
    "    # make predictions on the test data\n",
    "    predictions_ridge = ridge.predict(X_inner_test)\n",
    "    # compare the predictions to actual observations of the y\n",
    "    scores[str(alpha)] = accuracy_score(y_inner_test, predictions_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053dafb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e7172f",
   "metadata": {},
   "source": [
    "Most of the time, however, we want to not only compare **hyperparameters** for one **model family**, but we also want to compare accuracy across **different model families**, since we cannot know beforehand which model family will perform best. Lets import some other model families:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba5cd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d170ab5a",
   "metadata": {},
   "source": [
    "We can organise these models in a python dictionary, such that they **\"keys\"** of the dictionary tell us the **name of a classifier**, and the **\"values\"** of the dictionary holds the actual scikit-learn object. Note, that we can also use some model families multiple times with different hyperparameters. For example, we define three different **Support Vector Classifiers (SVC)**, each with a different **kernel** (this is one of the SVC's hyperparameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da2021",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"LogReg\": LogisticRegression(max_iter=1000),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=50),\n",
    "    \"linear_SVC\": SVC(kernel=\"linear\"),\n",
    "    \"rbf_SVC\": SVC(kernel=\"rbf\"),\n",
    "    \"poly_SVC\": SVC(kernel=\"poly\", degree=2),\n",
    "    \"DT\": DecisionTreeClassifier(),\n",
    "    \"RF\": RandomForestClassifier(),\n",
    "    \"GNB\": GaussianNB(),\n",
    "    \"Ridge\": RidgeClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fe283c",
   "metadata": {},
   "source": [
    "When looping through dictionaries, the key and corresponding value can be retrieved at the same time using the **items()** method. That means, in each iteration, we obtain the name and the object of a specific classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb08e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {}\n",
    "for classifier_name, classifier_object in classifiers.items():\n",
    "    # fit the model on the training data\n",
    "    classifier_object.fit(X_inner_train, y_inner_train)\n",
    "    # make predictions on the test data\n",
    "    classifier_predictions = classifier_object.predict(X_inner_test)\n",
    "    # compare the predictions to actual observations of the y\n",
    "    scores[classifier_name] = accuracy_score(y_inner_test, classifier_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1246fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09cf11",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Take the model that obtained the highest score in the model selection process, refit it on the model selection data (**X_model_selection, y_model_selection**) and test it on the holdout data (**X_holdout, y_holdout**). How does it perform now? Better, worse, or the same? What about other models that performed well (but not best) in the model selection process?\n",
    "2. Check out the documentation for some of the other model families. Can you see some more hyperparameters that sound interesting to you? Add more models with different hyperparameters. How do they affect accuracy on the test set? Do they perform better than the models already outlined above? How about on the hold-out set?\n",
    "\n",
    "# Additional Reading\n",
    "\n",
    "* [This article talks a bit more about train-test split evaluation](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/)\n",
    "\n",
    "* The scikit-learn user guide has some explanations and demonstrations of the above mentioned models [here](https://scikit-learn.org/stable/supervised_learning.html) and it usually links to some relevant papers or books as well.\n",
    "It is well worth trying to read and understand as much as possible about the individual algorithms you are planning to use in your research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba1c5c0",
   "metadata": {},
   "source": [
    "## Tuning Hyperparameters using Cross-Validation\n",
    "\n",
    "In the previous example we have compared different models regarding their accuracy on some test data, and selected the model that achieved the highest score on the test set. We have seen that a **model** can be defined by its **model family** and by the associated **hyperparameters** of that model family. We have seen a potential, naive strategy to select the best model for a problem at hand, but perhaps we have also noticed some problems with the proposed strategy. Importantly, we have noticed, that the performance of the selected model can change on the holdout set. That is, the model that we selected based on its performance on the test set, may not actually be the best model for the **general problem at hand** but may just be the model **that happened to work best on the test set**. That is, when using just one test set to assess accuracy of a model, we risk **\"overfitting\"** our model selection process to **this specific test set.** A better strategy for doing model selection may thus be to use cross-validation. \n",
    "\n",
    "Scikit-learn allows us to perform **model selection using a cross-validated gridsearch** using the **GridSearchCV** object. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7616ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0d860",
   "metadata": {},
   "source": [
    "Let's check out the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249bbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "?GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd168e",
   "metadata": {},
   "source": [
    "We can see many parameters, but there are 4 which we care about predominantly:\n",
    "\n",
    "1. **\"estimator\"** -> our sklearn estimator object (i.e. the model class)\n",
    "2. **\"param_grid\"** -> the grid of hyperparameters to search\n",
    "3. **\"scoring\"** -> which scoring metric to use\n",
    "4. **\"cv\"** -> the cross-validation scheme to use\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92034c2b",
   "metadata": {},
   "source": [
    "Let's for the moment go again with the example of the ridge classifier, for which we want to tune the alpha value. We can define the estimator as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3626afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b10401",
   "metadata": {},
   "source": [
    "The **param_grid** parameter typically is handed over as a **dictionary** in which the **keys** consist of the names of the parameters that are to be set for the estimator, and the **values** of the dictionary each yield an **iterable** (for example a **list**) of **possible candidate values** for each parameter.\n",
    "\n",
    "For example, for our ridge classifier, we can define a very simple grid, that only searches the value for one parameter (the alpha parameter) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.001, 0.01, 0.1, 1, 2, 10, 50, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc8ed9",
   "metadata": {},
   "source": [
    "The scoring parameter defines which scoring metric we care about, i.e. which scoring metric should be optimised by the grid search. It can be a string if the metric is already in-built in sklearn. For simplicity we will use \"accuracy\" here.\n",
    "To see what other metrics are available check out this: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd81b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abbebf",
   "metadata": {},
   "source": [
    "Lastly, the \"cv\" parameter can be any scikit-learn compatible cross-validation scheme. Here we will use a simple 5-fold cross-validation. We should also make sure that the KFold cv shuffles the data, but with a specific random state, so that the results are reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535c5f2",
   "metadata": {},
   "source": [
    "We can then initialise the GridSearchCV object, and fit it like any other scikit-learn estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4046fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearchcv = GridSearchCV(\n",
    "    estimator=ridge,\n",
    "    param_grid=param_grid_ridge,\n",
    "    scoring=scoring,\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "gridsearchcv.fit(X_model_selection, y_model_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580e1fa",
   "metadata": {},
   "source": [
    "The GridSearchCV has an **attribute** called **cv_results_** which we can access as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e704fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearchcv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072ab14",
   "metadata": {},
   "source": [
    "As you can see it is a dictionary with quite a lot of stuff, and somewhat difficult to read. However, it can be easily converted into a pandas dataframe for easier inspection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6464f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(gridsearchcv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342e342",
   "metadata": {},
   "source": [
    "We can see results for each of our 7 model candidates (remember, we used 7 alpha values to define our grid). That is, each row represents the results for 1 model candidate (for which you can see the parameters in the **\"params\"** column). Perhaps most interesting are the **\"mean_test_score\"** and **\"std_test_score\"**, which show us mean accuracy and the standard deviation across the different train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results[[\"params\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b3913",
   "metadata": {},
   "source": [
    "Conveniently, since the GridSearchCV had the **\"refit\"** parameter set to True, it already also selects the best scoring model and refits it on all of the data we gave it, so that we can now use it directly for further testing. We can check the best model and its parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearchcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6617fcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearchcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025cd75",
   "metadata": {},
   "source": [
    "Obviously, it's a RidgeClassifier, but we can also see that the alpha value fitted for the best model, corresponds to the alpha parameter for which we can see the highest score in the **cv_results** table. Let us now try to evaluate this best model on the final holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a839d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_predictions_gscv = gridsearchcv.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b86ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_holdout, holdout_predictions_gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9dd0e",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Do a similar grid search for other model families of your choice. Can you search grids with multiple hyperparameters (rather than just the one alpha parameter that we used in the example)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa294d9",
   "metadata": {},
   "source": [
    "# Additional Reading or Ressources\n",
    "\n",
    "\n",
    "The scikit-learn user guide has some good explanations on these topics in its user guide. For example you can look at:\n",
    "\n",
    "* [computing cross-validated metrics](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics)\n",
    "* [Tuning hyper-parameters using grid search](https://scikit-learn.org/stable/modules/grid_search.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5741ed",
   "metadata": {},
   "source": [
    "# Sklearn's pipeline\n",
    "\n",
    "Since we often want to apply preprocessing steps before fitting our models, and we want to do so in a cross-validation consistent way, so that we avoid data leakage and over-optimistic accuracy estimates, we need an easy way to chain different pipeline steps. The scikit-learn **pipeline** object provides an easy way to do just that.\n",
    "\n",
    "Check out its documentation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "?Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb6928",
   "metadata": {},
   "source": [
    "The main parameter we care about is the **\"steps\"** parameter. As the name (and the description) suggests, it is a list of the steps that we want to apply in our pipeline. Let's imagine for example, that we want to make a pipeline in which we first perform a **principal component analysis (PCA)** to extract 5 components, and then fit a Logistic Regression on only those 5 components. This may be quite complicated to implement in code especially if we also want to do some hyperparameter tuning in a cross-validated grid search, but combining the two steps in a pipeline makes it much more convenient.\n",
    "\n",
    "First let's prepare the PCA and the LogisticRegression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cdcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_5comps = PCA(n_components=5)\n",
    "log_reg = LogisticRegression(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e6fad",
   "metadata": {},
   "source": [
    "Now, the steps parameter of the pipeline actually takes a list of tuples. That is, each step of the pipeline is a tuple, that indicates the name of the step, and hands over the actual scikit-learn compatible object. Importantly, the last step must always be an estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3fa16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [(\"pca_5comps\", pca_5comps), (\"logistic_regression\", log_reg)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c456d7bc",
   "metadata": {},
   "source": [
    "We then simply hand this over to the pipeline, which we can then itself use like an estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bae4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(steps=steps)\n",
    "pipeline.fit(X_model_selection, y_model_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bf784",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_predictions = pipeline.predict(X_holdout)\n",
    "pipeline_score = accuracy_score(y_holdout, pipeline_predictions)\n",
    "pipeline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0378bff7",
   "metadata": {},
   "source": [
    "As you can see, accuracy is quite bad compared to what we achieved without the PCA. This is not really surprising since we are reducing the information from a few thousand features into 5 features only. Luckily we can perform cross-validated hyperparameter tuning now using GridSearchCV to find the \"optimal\" number of components. However, to define the **param_grid** we now need to specify the name of the pipeline step and the name of the parameter, separated by a double underscore such as:\n",
    "\n",
    "**\"stepname__parametername\"**\n",
    "\n",
    "We also want to compare the pipeline with PCA to a pipeline where PCA is not applied.\n",
    "We can do this by adding a second param_grid with the \"pca\": \"passthrough\" key-value pair. That is, we pass two param_grids in a list.\n",
    "\n",
    "Check out the example below and you can see it is really quite simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [(\"pca\", PCA()), (\"logistic_regression\", LogisticRegression(max_iter=1000))]\n",
    "pipeline = Pipeline(steps=steps)\n",
    "gridsearch_pipeline = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=[\n",
    "        {\"pca__n_components\": [5, 10, 50, 100, 250]},\n",
    "        {\"pca\": [\"passthrough\"]},\n",
    "    ],\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=100),\n",
    "    scoring=\"accuracy\",\n",
    ")\n",
    "gridsearch_pipeline.fit(X_model_selection, y_model_selection)\n",
    "cv_results = pd.DataFrame(gridsearch_pipeline.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf91f3",
   "metadata": {},
   "source": [
    "As you can see, the GridSearchCV results again display the scores for each hyperparameter, i.e. each amount of components we extracted in a PCA. Again we can evaluate the best model on our holdout dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82218e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_pipeline.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_predictions = gridsearch_pipeline.predict(X_holdout)\n",
    "pipeline_score = accuracy_score(y_holdout, holdout_predictions)\n",
    "pipeline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd881c",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "1. One very popular method of preprocessing where data leakage can happen quite easily is feature selection. Take one of the feature selection methods [in-built in scikit-learn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) and add it to a pipeline with a classifier of your choice. Can you do hyperparameter tuning for the feature selection process and the estimator simultaneously?\n",
    "Hint: check out the scikit-learn user guide to see [how you can use an F-test to select relevant features here](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ada44",
   "metadata": {},
   "source": [
    "# Bonus Material: How to perform a cross-validated grid search for both **model family** and **hyperparameters** simultaneously?\n",
    "\n",
    "Since we usually want to select the best model for a given problem from a set of different model families and their associated hyperparameters, you may now wonder how to do this with the GridSearchCV. We can see easily how it is done with one estimator, but it is not easy to see how to do this with a set of different estimators. This is another use case where the scikit-learn pipeline object can come in quite handy!\n",
    "\n",
    "We can initialise a pipeline with an estimator as a step, and then replace **this estimator** in the **pipeline** with **other type of estimators** using different parameter grids in our **GridSearchCV**, very similar to the way in which we tested the pipeline with and without PCA in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c08269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only define one step, we care only about the classifier and its hyperparameters.\n",
    "# We arbitrarily initialise the pipeline with ridge:\n",
    "pipeline = Pipeline(steps=[(\"classifier\", RidgeClassifier())])\n",
    "\n",
    "# parameters for the ridge classifier\n",
    "ridge_params = {\n",
    "    \"classifier\": [RidgeClassifier()], # parameters have to be handed over as iterables!\n",
    "    \"classifier__alpha\": [0.001, 0.01, 0.1, 1, 2, 10, 50, 100, 200, 500, 1000],\n",
    "}\n",
    "\n",
    "# parameters for the support vector classifier:\n",
    "svc_params = {\n",
    "    \"classifier\": [SVC()],\n",
    "    \"classifier__C\": [0.001, 0.01, 0.1, 1, 2, 10],\n",
    "    \"classifier__kernel\": [\"linear\", \"rbf\"],\n",
    "}\n",
    "\n",
    "# parameters for the random forest classifier:\n",
    "rf_params = {\n",
    "    \"classifier\": [RandomForestClassifier()],\n",
    "    \"classifier__n_estimators\": [10, 50, 100], \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ed6b2",
   "metadata": {},
   "source": [
    "After defining the estimators, the pipeline, and the parameter grids we can put it all together as follows and run the search. This may take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dba3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=[ridge_params, svc_params, rf_params],\n",
    "    scoring=\"accuracy\",\n",
    "    cv=kfold,\n",
    "    n_jobs=-1 # to speed up computation, '-1' means it will use all available CPU's\n",
    ")\n",
    "gridsearch_cv.fit(X_model_selection, y_model_selection)\n",
    "pd.DataFrame(gridsearch_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5ec0e",
   "metadata": {},
   "source": [
    "Let's evaluate again the best model on the holdout data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f72876",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_predictions = gridsearch_cv.predict(X_holdout)\n",
    "accuracy_score(y_holdout, gscv_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
