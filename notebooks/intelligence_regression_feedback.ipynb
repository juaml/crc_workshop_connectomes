{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0853fa4e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CRC Workshop: Machine Learning with Functional Connectivity (FC) Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba955ea",
   "metadata": {},
   "source": [
    "## Getting the data:\n",
    "\n",
    "Go to: https://github.com/juaml/crc_workshop_connectomes\n",
    "\n",
    "and follow the instructions in the README to get the data. Then go to the notebooks folder to open a new\n",
    "notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdac596",
   "metadata": {},
   "source": [
    "## The AOMIC Data\n",
    "\n",
    "First, let's load the data and inspect it a bit. The [AOMIC dataset](https://nilab-uva.github.io/AOMIC.github.io/) is a collection data obtained in three different studies (**PIOP1**, **PIOP2**, **ID1000**). Here, we will be only concerned with the data from the **ID1000** study, which aimed to collect 1000 fMRI scans during movie-watching. The next cell defines the path to all the **ID1000** specific data, and also adds the names of the two files we will be interested in. One of these files contains the preprocessed **functional connectivity (FC)** data, whereas the other file contains the important **demographic** information. Let's start by also loading the dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbd1570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e368676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to ID1000 data within the AOMIC datalad dataset\n",
    "ID1000_path = (\n",
    "    Path(\"..\") / \"aomic-fc\"/ \"junifer_storage\" / \n",
    "    \"JUNIFER_AOMIC_TSV_CONNECTOMES\" / \"ID1000\"\n",
    ")\n",
    "\n",
    "# Path to the demographics data file\n",
    "demographics_path = ID1000_path / \"ID1000_participants.tsv\"\n",
    "\n",
    "# Path to the connectomes data file\n",
    "# The name of this file is a bit of a mouthful but contains important\n",
    "# information\n",
    "connectomes_path = ID1000_path / (\n",
    "    \"ID1000_BOLD_parccortical-Schaefer100x17FSLMNI_\"\n",
    "    \"parcsubcortical-TianxS2x3TxMNInonlinear2009cAsym_\"\n",
    "    \"marker-empiricalFC_moviewatching.tsv.gz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd60e7",
   "metadata": {},
   "source": [
    "### Demographic Data\n",
    "\n",
    "Now that we have defined these paths let's load each file and look at them one by one. Let's start with the demographics. We will load it using pandas, and as you might see from the file extension, these are both **tsv** files and we will therefore load them using a tab as a delimiter. In addition, we will load the first column as the index of the dataframe as it happens to contain the subject ID's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f086b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows (subjects) x Columns (features)\n",
      "(928, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>handedness</th>\n",
       "      <th>BMI</th>\n",
       "      <th>education_level</th>\n",
       "      <th>background_SES</th>\n",
       "      <th>IST_fluid</th>\n",
       "      <th>IST_memory</th>\n",
       "      <th>IST_crystallised</th>\n",
       "      <th>IST_intelligence_total</th>\n",
       "      <th>...</th>\n",
       "      <th>sexual_attraction_M</th>\n",
       "      <th>sexual_attraction_F</th>\n",
       "      <th>gender_identity_M</th>\n",
       "      <th>gender_identity_F</th>\n",
       "      <th>religious_upbringing</th>\n",
       "      <th>religious_now</th>\n",
       "      <th>religious_importance</th>\n",
       "      <th>DWI_TR_run1</th>\n",
       "      <th>DWI_TR_run2</th>\n",
       "      <th>DWI_TR_run3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>participant_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>22.00</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>23</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>21.75</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>20</td>\n",
       "      <td>medium</td>\n",
       "      <td>5.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>25.25</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>31</td>\n",
       "      <td>high</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>22.50</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>22.25</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>23</td>\n",
       "      <td>high</td>\n",
       "      <td>4.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  age     sex handedness  BMI education_level  background_SES  \\\n",
       "participant_id                                                                  \n",
       "sub-0001        22.00  female      right   23          medium             2.0   \n",
       "sub-0002        21.75  female      right   20          medium             5.5   \n",
       "sub-0003        25.25  female      right   31            high             3.0   \n",
       "sub-0004        22.50  female      right   20            high             5.0   \n",
       "sub-0005        22.25    male      right   23            high             4.5   \n",
       "\n",
       "                IST_fluid  IST_memory  IST_crystallised  \\\n",
       "participant_id                                            \n",
       "sub-0001             77.0        49.0              33.0   \n",
       "sub-0002             97.0        63.0              39.0   \n",
       "sub-0003            122.0        67.0              38.0   \n",
       "sub-0004            149.0        69.0              52.0   \n",
       "sub-0005            112.0        57.0              43.0   \n",
       "\n",
       "                IST_intelligence_total  ...  sexual_attraction_M  \\\n",
       "participant_id                          ...                        \n",
       "sub-0001                         159.0  ...                  7.0   \n",
       "sub-0002                         199.0  ...                  7.0   \n",
       "sub-0003                         227.0  ...                  6.0   \n",
       "sub-0004                         270.0  ...                  6.0   \n",
       "sub-0005                         212.0  ...                  1.0   \n",
       "\n",
       "                sexual_attraction_F  gender_identity_M  gender_identity_F  \\\n",
       "participant_id                                                              \n",
       "sub-0001                        1.0                1.0                7.0   \n",
       "sub-0002                        1.0                2.0                7.0   \n",
       "sub-0003                        3.0                1.0                6.0   \n",
       "sub-0004                        2.0                1.0                7.0   \n",
       "sub-0005                        7.0                6.0                1.0   \n",
       "\n",
       "                religious_upbringing  religious_now  religious_importance  \\\n",
       "participant_id                                                              \n",
       "sub-0001                          no            yes                   2.0   \n",
       "sub-0002                          no             no                   NaN   \n",
       "sub-0003                          no             no                   NaN   \n",
       "sub-0004                         yes             no                   NaN   \n",
       "sub-0005                          no             no                   NaN   \n",
       "\n",
       "                DWI_TR_run1  DWI_TR_run2  DWI_TR_run3  \n",
       "participant_id                                         \n",
       "sub-0001              6.312        6.312        6.312  \n",
       "sub-0002                NaN        6.311        6.311  \n",
       "sub-0003              6.312        6.312        6.312  \n",
       "sub-0004              6.311        6.311        6.311  \n",
       "sub-0005              6.311        6.311        6.311  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographics = pd.read_csv(demographics_path, sep=\"\\t\", index_col=0)\n",
    "print(\"Rows (subjects) x Columns (features)\")\n",
    "print(demographics.shape)\n",
    "demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd6b297",
   "metadata": {},
   "source": [
    "We can see some of the standard demographic variables, like \"age\", \"sex\", \"BMI\", and so on. As you might be able to tell, however, this file not *only* contains \"demographic\" information but also some other participant data, as for example cognitive measurements (e.g. \"IST_memory\", \"IST_fluid\").\n",
    "\n",
    "### Connectomes\n",
    "\n",
    "Let us now check the connectomes out to see for which subjects we have preprocessed functional connectivity data available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e404919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows (subjects) x Columns (features)\n",
      "(877, 8646)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LH_VisCent_ExStr_2~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisCent_Striate_1~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisCent_Striate_1~LH_VisCent_ExStr_2</th>\n",
       "      <th>LH_VisCent_ExStr_3~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisCent_ExStr_3~LH_VisCent_ExStr_2</th>\n",
       "      <th>LH_VisCent_ExStr_3~LH_VisCent_Striate_1</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_2</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_Striate_1</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_3</th>\n",
       "      <th>...</th>\n",
       "      <th>pCAU-lh~THA-VP-lh</th>\n",
       "      <th>pCAU-lh~THA-VA-lh</th>\n",
       "      <th>pCAU-lh~THA-DA-lh</th>\n",
       "      <th>pCAU-lh~NAc-shell-lh</th>\n",
       "      <th>pCAU-lh~NAc-core-lh</th>\n",
       "      <th>pCAU-lh~pGP-lh</th>\n",
       "      <th>pCAU-lh~aGP-lh</th>\n",
       "      <th>pCAU-lh~aPUT-lh</th>\n",
       "      <th>pCAU-lh~pPUT-lh</th>\n",
       "      <th>pCAU-lh~aCAU-lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>0.533892</td>\n",
       "      <td>0.638701</td>\n",
       "      <td>0.560450</td>\n",
       "      <td>0.789158</td>\n",
       "      <td>0.466875</td>\n",
       "      <td>0.539399</td>\n",
       "      <td>0.677232</td>\n",
       "      <td>0.366597</td>\n",
       "      <td>0.361958</td>\n",
       "      <td>0.564919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>0.366347</td>\n",
       "      <td>0.327794</td>\n",
       "      <td>-0.069095</td>\n",
       "      <td>0.468919</td>\n",
       "      <td>-0.071477</td>\n",
       "      <td>0.040889</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.173870</td>\n",
       "      <td>0.664122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>0.641335</td>\n",
       "      <td>0.663375</td>\n",
       "      <td>0.545326</td>\n",
       "      <td>0.629451</td>\n",
       "      <td>0.187963</td>\n",
       "      <td>0.390782</td>\n",
       "      <td>0.670098</td>\n",
       "      <td>0.116997</td>\n",
       "      <td>0.397939</td>\n",
       "      <td>0.742349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048029</td>\n",
       "      <td>0.294397</td>\n",
       "      <td>0.247182</td>\n",
       "      <td>0.050190</td>\n",
       "      <td>0.069237</td>\n",
       "      <td>-0.133454</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.265424</td>\n",
       "      <td>0.147957</td>\n",
       "      <td>0.587463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>0.657928</td>\n",
       "      <td>0.742613</td>\n",
       "      <td>0.453348</td>\n",
       "      <td>0.860049</td>\n",
       "      <td>0.523853</td>\n",
       "      <td>0.591320</td>\n",
       "      <td>0.770457</td>\n",
       "      <td>0.418997</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.725303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145307</td>\n",
       "      <td>0.240758</td>\n",
       "      <td>-0.014584</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.397896</td>\n",
       "      <td>0.112837</td>\n",
       "      <td>0.292399</td>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.224996</td>\n",
       "      <td>0.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>0.788957</td>\n",
       "      <td>0.694655</td>\n",
       "      <td>0.660015</td>\n",
       "      <td>0.732351</td>\n",
       "      <td>0.517771</td>\n",
       "      <td>0.387978</td>\n",
       "      <td>0.620954</td>\n",
       "      <td>0.283812</td>\n",
       "      <td>0.426040</td>\n",
       "      <td>0.622988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288743</td>\n",
       "      <td>0.276673</td>\n",
       "      <td>0.355181</td>\n",
       "      <td>0.250076</td>\n",
       "      <td>0.216825</td>\n",
       "      <td>-0.064241</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>0.379170</td>\n",
       "      <td>0.226584</td>\n",
       "      <td>0.388584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>0.594372</td>\n",
       "      <td>0.761742</td>\n",
       "      <td>0.667648</td>\n",
       "      <td>0.791777</td>\n",
       "      <td>0.467393</td>\n",
       "      <td>0.515712</td>\n",
       "      <td>0.285025</td>\n",
       "      <td>-0.306764</td>\n",
       "      <td>0.179985</td>\n",
       "      <td>0.250133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009222</td>\n",
       "      <td>0.393067</td>\n",
       "      <td>0.032954</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.289458</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.426828</td>\n",
       "      <td>0.701249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8646 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LH_VisCent_ExStr_2~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                               0.533892   \n",
       "sub-0002                               0.641335   \n",
       "sub-0003                               0.657928   \n",
       "sub-0004                               0.788957   \n",
       "sub-0005                               0.594372   \n",
       "\n",
       "          LH_VisCent_Striate_1~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                                 0.638701   \n",
       "sub-0002                                 0.663375   \n",
       "sub-0003                                 0.742613   \n",
       "sub-0004                                 0.694655   \n",
       "sub-0005                                 0.761742   \n",
       "\n",
       "          LH_VisCent_Striate_1~LH_VisCent_ExStr_2  \\\n",
       "sub-0001                                 0.560450   \n",
       "sub-0002                                 0.545326   \n",
       "sub-0003                                 0.453348   \n",
       "sub-0004                                 0.660015   \n",
       "sub-0005                                 0.667648   \n",
       "\n",
       "          LH_VisCent_ExStr_3~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                               0.789158   \n",
       "sub-0002                               0.629451   \n",
       "sub-0003                               0.860049   \n",
       "sub-0004                               0.732351   \n",
       "sub-0005                               0.791777   \n",
       "\n",
       "          LH_VisCent_ExStr_3~LH_VisCent_ExStr_2  \\\n",
       "sub-0001                               0.466875   \n",
       "sub-0002                               0.187963   \n",
       "sub-0003                               0.523853   \n",
       "sub-0004                               0.517771   \n",
       "sub-0005                               0.467393   \n",
       "\n",
       "          LH_VisCent_ExStr_3~LH_VisCent_Striate_1  \\\n",
       "sub-0001                                 0.539399   \n",
       "sub-0002                                 0.390782   \n",
       "sub-0003                                 0.591320   \n",
       "sub-0004                                 0.387978   \n",
       "sub-0005                                 0.515712   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                                  0.677232   \n",
       "sub-0002                                  0.670098   \n",
       "sub-0003                                  0.770457   \n",
       "sub-0004                                  0.620954   \n",
       "sub-0005                                  0.285025   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_2  \\\n",
       "sub-0001                                  0.366597   \n",
       "sub-0002                                  0.116997   \n",
       "sub-0003                                  0.418997   \n",
       "sub-0004                                  0.283812   \n",
       "sub-0005                                 -0.306764   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_Striate_1  \\\n",
       "sub-0001                                    0.361958   \n",
       "sub-0002                                    0.397939   \n",
       "sub-0003                                    0.645827   \n",
       "sub-0004                                    0.426040   \n",
       "sub-0005                                    0.179985   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_3  ...  pCAU-lh~THA-VP-lh  \\\n",
       "sub-0001                                  0.564919  ...           0.072425   \n",
       "sub-0002                                  0.742349  ...          -0.048029   \n",
       "sub-0003                                  0.725303  ...          -0.145307   \n",
       "sub-0004                                  0.622988  ...           0.288743   \n",
       "sub-0005                                  0.250133  ...          -0.009222   \n",
       "\n",
       "          pCAU-lh~THA-VA-lh  pCAU-lh~THA-DA-lh  pCAU-lh~NAc-shell-lh  \\\n",
       "sub-0001           0.366347           0.327794             -0.069095   \n",
       "sub-0002           0.294397           0.247182              0.050190   \n",
       "sub-0003           0.240758          -0.014584              0.125811   \n",
       "sub-0004           0.276673           0.355181              0.250076   \n",
       "sub-0005           0.393067           0.032954              0.220143   \n",
       "\n",
       "          pCAU-lh~NAc-core-lh  pCAU-lh~pGP-lh  pCAU-lh~aGP-lh  \\\n",
       "sub-0001             0.468919       -0.071477        0.040889   \n",
       "sub-0002             0.069237       -0.133454        0.011535   \n",
       "sub-0003             0.397896        0.112837        0.292399   \n",
       "sub-0004             0.216825       -0.064241        0.045752   \n",
       "sub-0005             0.289458        0.012017        0.073975   \n",
       "\n",
       "          pCAU-lh~aPUT-lh  pCAU-lh~pPUT-lh  pCAU-lh~aCAU-lh  \n",
       "sub-0001         0.212324         0.173870         0.664122  \n",
       "sub-0002         0.265424         0.147957         0.587463  \n",
       "sub-0003         0.473047         0.224996         0.711400  \n",
       "sub-0004         0.379170         0.226584         0.388584  \n",
       "sub-0005         0.515776         0.426828         0.701249  \n",
       "\n",
       "[5 rows x 8646 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connectomes = pd.read_csv(connectomes_path, sep=\"\\t\", index_col=0, compression=\"gzip\")\n",
    "print(\"Rows (subjects) x Columns (features)\")\n",
    "print(connectomes.shape)\n",
    "connectomes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f5a83",
   "metadata": {},
   "source": [
    "In this dataframe again, **each row** corresponds to *one subject* from the study. **Each column** represents a *unique pairwise relationship* between two brain areas. Since a brain parcellation with **N** areas results in an **NxN** symmetric correlation matrix per subject, one half of a subject's matrix is discarded. Similarly, the diagonal of this correlation matrix is also typically discarded as the correlation of an area with itself is always 1. The remaining entries can be stacked and result in one row of this dataframe. Thus, each row contains **N x (N-1) / 2** entries. In our case, the connectomes have **100 x (100 - 1) / 2 = 8646** columns. This concept is illustrated by the graphic below:\n",
    "\n",
    "![title](images/connectomes.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb69a1e6",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "\n",
    "Let's make sure that we identify any missing information or values ('NaN') in the functional connectivity data and remove subjects with any 'NaN' entries.\n",
    "\n",
    "The pandas **isna()** method will check for each entry in the dataframe whether it is 'NaN' or not. That is, if an entry is 'NaN' it will return True and otherwise it will return False. We can use this to identify the indices (i.e. subjects) for which there are 'NaN' entries by combining it with the **any()** method provided by pandas.\n",
    "\n",
    "First see the output from **isna()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92e0553c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if any value of the matix is NaN\n",
    "isna = connectomes.isna()\n",
    "# Check if there is any missing values in each row\n",
    "isna_any = isna.any(axis=1)\n",
    "# If the sum is 0, no missing value. If not, this number will represent the number of \n",
    "# rows (subjects) that have missing information. \n",
    "isna_any.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22fd78c",
   "metadata": {},
   "source": [
    "The output (\"0\") shows us that there aren't any 'NaN' values, so we can simply proceed with the data we have here. Let us therefore now subset the demographic data for which we have connectomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb482e7",
   "metadata": {},
   "source": [
    "As you can see from the previous cells, the demographics dataset contains 928 rows (i.e. subjects), whereas the connectome dataframe contains only 877 rows. Let us for further analyses only select subjects for which we actually have connectomes. We will keep the subjects in the demographics dataset that also have connectome information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8490e76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows (subjects) x Columns (features)\n",
      "(877, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>handedness</th>\n",
       "      <th>BMI</th>\n",
       "      <th>education_level</th>\n",
       "      <th>background_SES</th>\n",
       "      <th>IST_fluid</th>\n",
       "      <th>IST_memory</th>\n",
       "      <th>IST_crystallised</th>\n",
       "      <th>IST_intelligence_total</th>\n",
       "      <th>...</th>\n",
       "      <th>sexual_attraction_M</th>\n",
       "      <th>sexual_attraction_F</th>\n",
       "      <th>gender_identity_M</th>\n",
       "      <th>gender_identity_F</th>\n",
       "      <th>religious_upbringing</th>\n",
       "      <th>religious_now</th>\n",
       "      <th>religious_importance</th>\n",
       "      <th>DWI_TR_run1</th>\n",
       "      <th>DWI_TR_run2</th>\n",
       "      <th>DWI_TR_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>22.00</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>23</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>21.75</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>20</td>\n",
       "      <td>medium</td>\n",
       "      <td>5.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>25.25</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>31</td>\n",
       "      <td>high</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>22.50</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>22.25</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>23</td>\n",
       "      <td>high</td>\n",
       "      <td>4.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age     sex handedness  BMI education_level  background_SES  \\\n",
       "sub-0001  22.00  female      right   23          medium             2.0   \n",
       "sub-0002  21.75  female      right   20          medium             5.5   \n",
       "sub-0003  25.25  female      right   31            high             3.0   \n",
       "sub-0004  22.50  female      right   20            high             5.0   \n",
       "sub-0005  22.25    male      right   23            high             4.5   \n",
       "\n",
       "          IST_fluid  IST_memory  IST_crystallised  IST_intelligence_total  \\\n",
       "sub-0001       77.0        49.0              33.0                   159.0   \n",
       "sub-0002       97.0        63.0              39.0                   199.0   \n",
       "sub-0003      122.0        67.0              38.0                   227.0   \n",
       "sub-0004      149.0        69.0              52.0                   270.0   \n",
       "sub-0005      112.0        57.0              43.0                   212.0   \n",
       "\n",
       "          ...  sexual_attraction_M  sexual_attraction_F  gender_identity_M  \\\n",
       "sub-0001  ...                  7.0                  1.0                1.0   \n",
       "sub-0002  ...                  7.0                  1.0                2.0   \n",
       "sub-0003  ...                  6.0                  3.0                1.0   \n",
       "sub-0004  ...                  6.0                  2.0                1.0   \n",
       "sub-0005  ...                  1.0                  7.0                6.0   \n",
       "\n",
       "          gender_identity_F  religious_upbringing  religious_now  \\\n",
       "sub-0001                7.0                    no            yes   \n",
       "sub-0002                7.0                    no             no   \n",
       "sub-0003                6.0                    no             no   \n",
       "sub-0004                7.0                   yes             no   \n",
       "sub-0005                1.0                    no             no   \n",
       "\n",
       "          religious_importance  DWI_TR_run1  DWI_TR_run2  DWI_TR_run3  \n",
       "sub-0001                   2.0        6.312        6.312        6.312  \n",
       "sub-0002                   NaN          NaN        6.311        6.311  \n",
       "sub-0003                   NaN        6.312        6.312        6.312  \n",
       "sub-0004                   NaN        6.311        6.311        6.311  \n",
       "sub-0005                   NaN        6.311        6.311        6.311  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled_demographics = demographics.loc[connectomes.index]\n",
    "print(\"Rows (subjects) x Columns (features)\")\n",
    "print(subsampled_demographics.shape)\n",
    "subsampled_demographics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980d92e5",
   "metadata": {},
   "source": [
    "The indexing using the **.loc()** method importantly also ensures that the rows in both dataframes are in the same order which will be important later when we convert them to numpy arrays, a data structure that scikit-learn understands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed3214",
   "metadata": {},
   "source": [
    "### Exploring our sample:\n",
    "\n",
    "Now that the samples in the connectome data and the demographics data are matched, let's take a quick look at sex and age to get an overview of our sample.\n",
    "\n",
    "The **value_counts()** method takes a pandas series (i.e. a column from the dataframe) and counts the number of times each possible value is contained in the column. This is a good way of discovering what values are possible for a specific variable, and how many instances there are for each value. This is useful for example when looking at categorical variables, for example \"sex\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee755abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    454\n",
       "male      423\n",
       "Name: sex, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled_demographics[\"sex\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413de870",
   "metadata": {},
   "source": [
    "The **plot.hist()** pandas method provides a quick way of making a histogram that we can also group by \"sex\" to look at each distribution seperately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e340d0d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot: title={'center': 'female'}, ylabel='Frequency'>,\n",
       "       <AxesSubplot: title={'center': 'male'}, ylabel='Frequency'>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsampled_demographics.plot.hist(column=[\"age\"], by=\"sex\", figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fa930a",
   "metadata": {},
   "source": [
    "As you can see, the age range is quite narrow, and limited to young people. This is a common problem in neuroimaging or psychology studies, which often sample students from their universities for convenience. It is always good to be aware of these limitations before starting any complicated machine learning pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce09c14",
   "metadata": {},
   "source": [
    "## Preparing the target\n",
    "\n",
    "Now, let's try to build a model that can predict a measure of intelligence given a functional connectome. That is, the connectomes will be the **features ('X')** and the measure of intelligence will be the **target ('y')**. Let us first take a quick look at our target and its distribution. The **.describe()** method that pandas provides can be used to get a quick look over the relevant descriptive statistics that we may care about:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86ccfac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    875.000000\n",
       "mean     200.400000\n",
       "std       40.765557\n",
       "min       68.000000\n",
       "25%      172.000000\n",
       "50%      205.000000\n",
       "75%      230.000000\n",
       "max      296.000000\n",
       "Name: IST_intelligence_total, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled_demographics[\"IST_intelligence_total\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dff35c",
   "metadata": {},
   "source": [
    "As we did before, we can quickly plot the histogram of our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458a222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<AxesSubplot: title={'center': 'female'}, ylabel='Frequency'>,\n",
       "       <AxesSubplot: title={'center': 'male'}, ylabel='Frequency'>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl4AAAHiCAYAAAA5wcIVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAAsTAAALEwEAmpwYAAAw/0lEQVR4nO3de5RdZZnv+++TBKhwDZd0RIJUoYgkBCKpIIIIbZr7/TSw49ZtQEbSCEinZatRPC1eB2haWmW32yhCpKEFCjvc9x5A6G4VOlCBInKRA2jQggghIUDMhUCe88eaSZehKrWqqDVX1cr3M0aNWvP+rLyZyW/M951zRmYiSZKk2htW7wIkSZK2FAYvSZKkkhi8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCQNGRGxb0R0RMRrEXFhicfNiHhPWceT1LhG1LsASeqDzwH3ZubEehciSf3hFS9JQ8lewGP1LkKS+svgJWlIiIj5wF8CV0TEyqLbcXZE/D4iXoiI/x0RI4t1j4yIzoj4XES8GBFLIuLUiDg+Iv6/iFgeEV/ssu+DI+L+iFhRrHtFRGzdQx3b9HRcSeqNwUvSkJCZHwF+AVyQmdsD5wLvBSYC7wH2AP6+yybvAJq6zP8R8HFgEnA48P9GREux7pvA3wG7AR8EpgDn9VDKpb0cV5J6FL6rUdJQERH/BvwzcCWwEjggM58pln0QuC4zWyLiSOBOYPvMfDMidgBeBQ7JzAXF+guBr2XmvG6OMxM4IjNPK6YT2Ad4ZnPHrdHXltRAHFwvaSgaDWwLLIyIDfMCGN5lnWWZ+WbxeXXx+4Uuy1cD2wNExHuB7wCtxX5HAAv7eVxJ6pFdjZKGopeoBKfxmTmq+Nmp6ILsjx8AvwH2ycwdgS9SCVS1Pq6kLYzBS9KQk5nrqYzZujwi/gIgIvaIiGP6ucsNXZErI+J9wKdKOq6kLYzBS9JQ9XngaeA/I+JV4G5g337u638C/x14jUqwur6k40rawji4XpIkqSRe8ZIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKYvCSJEkqyZB4cv1uu+2Wzc3N9S5DkiSpVwsXLnwpM0d3t2xIBK/m5mba29vrXYYkSVKvIuLZnpbZ1ShJklQSg5ckSVJJDF6SJEklGRJjvCRJqrV169bR2dnJmjVr6l2KhoimpibGjh3LVlttVfU2Bi9JkoDOzk522GEHmpubiYh6l6NBLjNZtmwZnZ2dtLS0VL2dXY2SJAFr1qxh1113NXSpKhHBrrvu2ucrpAYvSZIKhi71RX/+vtjVKEkNoHnW7fUuYUAsvvSEepcg1ZTBS5Kkbgx0mK0mVG6//fasXLmS9evXM3PmTObPn09E0NTUxA033MDUqVNZu3Yty5cvZ/Xq1eyxxx4AzJs3j+7e8HL88cdz3XXXMWrUqB6PefXVV3P00Ufzzne+c7O1nXXWWZx44omcfvrpHHnkkcyePZvW1taqjlEP8+bN473vfS/jxo3b7Hr9+f5vh8FLkqRB5vrrr+f5559n0aJFDBs2jM7OTrbbbjsWLFgAVMJCe3s7V1xxxWb3c8cdd/R6rKuvvpr999+/1+Dxdo5RD/PmzePEE0+sKni9ne/fV47xkiRpkFmyZAm77747w4ZV/pseO3YsO++8c5/309zczEsvvcTixYvZb7/9mD59OuPHj+foo49m9erVtLW10d7ezsc+9jEmTpzI6tWrWbhwIUcccQSTJk3imGOOYcmSJVUdA+BrX/sa++67Lx/60If46Ec/yuzZswF45plnOPbYY5k0aRKHH344v/nNb4DKVaQLL7yQQw89lL333pu2traN+73sssuYMGECBx54ILNmzdrsfjZ13333ccstt/DZz36WiRMn8swzz9DR0cEhhxzCAQccwGmnncbLL7/c7ff/6le/yuTJk9l///2ZMWMGmdnnP/fNMXhJkjTInHnmmdx6661MnDiRiy66iIcffvht7/Opp57i/PPP57HHHmPUqFHcdNNNnH766bS2tnLttdfS0dHBiBEj+PSnP01bWxsLFy7kk5/8JBdffHFV+3/wwQe56aabeOSRR7jzzjv/7B3LM2bM4Pvf/z4LFy5k9uzZnHfeeRuXLVmyhF/+8pfcdtttGwPWnXfeyc0338yCBQt45JFH+NznPtfrfro69NBDOfnkk/n2t79NR0cH7373u/nEJz7BZZddxqJFi5gwYQJf+cpX3vL9R44cyQUXXMCDDz7Io48+yurVq7ntttv6+0feLbsaJUkaZMaOHcuTTz7J/PnzmT9/PlOmTOHGG29kypQp/d5nS0sLEydOBGDSpEksXrz4Les8+eSTPProoxx11FEAvPnmm+y+++5V7f9Xv/oVp5xyCk1NTTQ1NXHSSScBsHLlSu677z7OOOOMjeuuXbt24+dTTz2VYcOGMW7cOF544QUA7r77bs4++2y23XZbAHbZZZde97M5r7zyCitWrOCII44AYNq0aX+2n67uvfdevvWtb7Fq1SqWL1/O+PHjN36XgWDwkiRpENpmm2047rjjOO644xgzZgzz5s17W8Frm2222fh5+PDhrF69+i3rZCbjx4/n/vvv7/dxNrV+/XpGjRpFR0dHr3Vtrluvt/0MhDVr1nDeeefR3t7OnnvuySWXXDLgbzKwq1GSpEHmoYce4vnnnwcqgWPRokXstddeNTnWDjvswGuvvQbAvvvuy9KlSzcGr3Xr1vHYY49VtZ/DDjuMW2+9lTVr1rBy5cqNXXQ77rgjLS0t3HjjjUAlXD3yyCOb3ddRRx3FVVddxapVqwBYvnx5n/fT9XvttNNO7LzzzvziF78A4Jprrtl49avrehtC1m677cbKlSv/bMzZQPGKlyRJ3ajnM8VefPFFpk+fvrEr7eCDD+aCCy6oybHOOusszj33XEaOHMn9999PW1sbF154Ia+88gpvvPEGM2fOZPz48b3uZ/LkyZx88skccMABjBkzhgkTJrDTTjsBcO211/KpT32Kr3/966xbt46pU6dy4IEH9rivY489lo6ODlpbW9l66605/vjj+eY3v9mn/UydOpXp06fzve99j7a2NubOncu5557LqlWr2Hvvvbnqqqu6/f7Tp09n//335x3veAeTJ0/ux5/o5sVAj9avhdbW1uw6SE+S9Od8gOrb98QTT7DffvvV7fiNYOXKlWy//fasWrWKD3/4w8yZM4eDDjqo3mXVVHd/byJiYWa2dre+V7wkSdKAmDFjBo8//jhr1qxh2rRpDR+6+sPgJUnSEPeBD3zgLXf4XXPNNUyYMKHUOq677rpSjwfwjW98Y+O4rw3OOOOMqh+DUTaDlyRJQ9yGJ9pviS6++OJBG7K6412NkiQVhsK4Zw0e/fn7YvCSJAloampi2bJlhi9VJTNZtmwZTU1NfdrOrkZJkqg8Lb6zs5OlS5fWuxQNEU1NTYwdO7ZP2xi8JEkCttpqK1paWupdhhqcwUtSnzTK86Kgvs+MkrRlcoyXJElSSQxekiRJJTF4SZIklaSmwSsi/i4iHouIRyPiXyKiKSJaImJBRDwdEddHxNa1rEGSJGmwqFnwiog9gAuB1szcHxgOTAUuAy7PzPcALwPn1KoGSZKkwaTWXY0jgJERMQLYFlgCfARoK5bPBU6tcQ2SJEmDQs2CV2Y+B8wGfk8lcL0CLARWZOYbxWqdwB61qkGSJGkwqdlzvCJiZ+AUoAVYAdwIHNuH7WcAMwDe9a531aBCSVu6RnommaShoZZdjX8F/C4zl2bmOuDnwGHAqKLrEWAs8Fx3G2fmnMxszczW0aNH17BMSZKkctQyeP0eOCQito2IAKYAjwP3AqcX60wDbq5hDZIkSYNGLcd4LaAyiP4h4NfFseYAnwc+ExFPA7sCV9aqBkmSpMGkpu9qzMwvA1/eZPZvgYNreVxJkqTByCfXS5IklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJXE4CVJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJXE4CVJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJXE4CVJklQSg5ckSVJJDF6SJEklqWnwiohREdEWEb+JiCci4oMRsUtE3BURTxW/d65lDZIkSYNFra94fRf4P5n5PuBA4AlgFnBPZu4D3FNMS5IkNbyaBa+I2An4MHAlQGa+npkrgFOAucVqc4FTa1WDJEnSYFLLK14twFLgqoh4OCJ+HBHbAWMyc0mxzh+BMTWsQZIkadCoKnhFxIR+7HsEcBDwg8x8P/AnNulWzMwEsodjzoiI9ohoX7p0aT8OL0mSNLhUe8XrnyLigYg4r+hCrEYn0JmZC4rpNipB7IWI2B2g+P1idxtn5pzMbM3M1tGjR1d5SEmSpMGrquCVmYcDHwP2BBZGxHURcVQv2/wR+ENE7FvMmgI8DtwCTCvmTQNu7k/hkiRJQ82IalfMzKci4ktAO/A94P0REcAXM/PnPWz2aeDaiNga+C1wNpWwd0NEnAM8C5z5dr6AJEnSUFFV8IqIA6iEphOAu4CTMvOhiHgncD/QbfDKzA6gtZtFU/pVrSRJ0hBW7RWv7wM/pnJ1a/WGmZn5fHEVTJIkSb2oNnidAKzOzDcBImIY0JSZqzLzmppVJzWI5lm317sESdIgUO1djXcDI7tMb1vMkyRJUpWqDV5Nmblyw0TxedvalCRJktSYqg1ef4qIgzZMRMQkYPVm1pckSdImqh3jNRO4MSKeBwJ4B/DfalWUJElSI6oqeGXmgxHxPmDDw1CfzMx1tStLkiSp8VT9AFVgMtBcbHNQRJCZP61JVZIkSQ2o2geoXgO8G+gA3ixmJ2DwkiRJqlK1V7xagXGZmbUsRpIkqZFVe1fjo1QG1EuSJKmfqr3itRvweEQ8AKzdMDMzT65JVZIkSQ2o2uB1SS2LkCRJ2hJU+ziJf4+IvYB9MvPuiNgWGF7b0iRJkhpLVWO8ImI60Ab8sJi1BzCvRjVJkiQ1pGq7Gs8HDgYWAGTmUxHxFzWrSpK0RWqedXu9Sxgwiy89od4laBCq9q7GtZn5+oaJiBhB5TlekiRJqlK1wevfI+KLwMiIOAq4Ebi1dmVJkiQ1nmqD1yxgKfBr4G+AO4Av1aooSZKkRlTtXY3rgR8VP5IkSeqHat/V+Du6GdOVmXsPeEWSJEkNqi/vatygCTgD2GXgy5EkSWpcVY3xysxlXX6ey8x/BLxPVpIkqQ+q7Wo8qMvkMCpXwKq9WiZJkiSqD0//0OXzG8Bi4MwBr0aSJKmBVXtX41/WuhBJkqRGV21X42c2tzwzvzMw5UiSJDWuvtzVOBm4pZg+CXgAeKoWRUmSJDWiaoPXWOCgzHwNICIuAW7PzI/XqjBJkqRGU+0rg8YAr3eZfr2YJ0mSpCpVe8Xrp8ADEfGvxfSpwNyaVCRJktSgqr2r8RsRcSdweDHr7Mx8uHZlSZIkNZ5quxoBtgVezczvAp0R0VLNRhExPCIejojbiumWiFgQEU9HxPURsXU/6pYkSRpyqgpeEfFl4PPAF4pZWwH/XOUx/hZ4osv0ZcDlmfke4GXgnCr3I0mSNKRVe8XrNOBk4E8Amfk8sENvG0XEWCrvdPxxMR3AR4C2YpW5VMaLSZIkNbxqg9frmZlAAkTEdlVu94/A54D1xfSuwIrMfKOY7gT2qHJfkiRJQ1q1weuGiPghMCoipgN3Az/a3AYRcSLwYmYu7E9hETEjItojon3p0qX92YUkSdKg0utdjUX34PXA+4BXgX2Bv8/Mu3rZ9DDg5Ig4HmgCdgS+SyW8jSiueo0Fnutu48ycA8wBaG1tzeq+jiRJ0uDVa/DKzIyIOzJzAtBb2Oq63RcoBuNHxJHA/8zMj0XEjcDpwM+AacDN/ahbW4jmWbfXuwRJkgZMtV2ND0XE5AE65ueBz0TE01TGfF05QPuVJEka1Kp9cv0HgI9HxGIqdzYGlYthB1SzcWb+G/BvxeffAgf3tVBJkqShbrPBKyLelZm/B44pqR5JkqSG1dsVr3nAQZn5bETclJl/XUJNkiRJDam3MV7R5fPetSxEkiSp0fUWvLKHz5IkSeqj3roaD4yIV6lc+RpZfIb/Gly/Y02rkyRJaiCbDV6ZObysQiRJkhpdtc/xkiRJ0ttk8JIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKYvCSJEkqicFLkiSpJAYvSZKkkhi8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKYvCSJEkqicFLkiSpJAYvSZKkkhi8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSrJiFrtOCL2BH4KjAESmJOZ342IXYDrgWZgMXBmZr5cqzokSaqH5lm317uEAbP40hPqXULDqOUVrzeAizJzHHAIcH5EjANmAfdk5j7APcW0JElSw6tZ8MrMJZn5UPH5NeAJYA/gFGBusdpc4NRa1SBJkjSYlDLGKyKagfcDC4AxmbmkWPRHKl2RkiRJDa/mwSsitgduAmZm5qtdl2VmUhn/1d12MyKiPSLaly5dWusyJUmSaq6mwSsitqISuq7NzJ8Xs1+IiN2L5bsDL3a3bWbOyczWzGwdPXp0LcuUJEkqRc2CV0QEcCXwRGZ+p8uiW4BpxedpwM21qkGSJGkwqdnjJIDDgP8B/DoiOop5XwQuBW6IiHOAZ4Eza1iDJEnSoFGz4JWZvwSih8VTanVcSZKkwcon10uSJJXE4CVJklQSg5ckSVJJajm4XnXSSO8HkySpkXjFS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKYvCSJEkqicFLkiSpJAYvSZKkkhi8JEmSSuK7Ggu+31CSJNWaV7wkSZJKYvCSJEkqicFLkiSpJAYvSZKkkhi8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKYvCSJEkqicFLkiSpJAYvSZKkkoyox0Ej4ljgu8Bw4MeZeWk96pAkSb1rnnV7vUsYEIsvPaHeJZR/xSsihgP/CzgOGAd8NCLGlV2HJElS2erR1Xgw8HRm/jYzXwd+BpxShzokSZJKVY/gtQfwhy7TncU8SZKkhlaXMV7ViIgZwIxicmVEPFnPegbIbsBL9S5CpbG9txy29ZbF9h6i4rJ+bdaf9t6rpwX1CF7PAXt2mR5bzPszmTkHmFNWUWWIiPbMbK13HSqH7b3lsK23LLb3lmWg27seXY0PAvtEREtEbA1MBW6pQx2SJEmlKv2KV2a+EREXAP+XyuMkfpKZj5VdhyRJUtnqMsYrM+8A7qjHseusobpO1Svbe8thW29ZbO8ty4C2d2TmQO5PkiRJPfCVQZIkSSUxeA2giPhJRLwYEY92mbdLRNwVEU8Vv3cu5kdEfC8ino6IRRFxUP0qV1/10NaXRMRzEdFR/BzfZdkXirZ+MiKOqU/V6q+I2DMi7o2IxyPisYj422K+53eD2Uxbe343oIhoiogHIuKRor2/UsxviYgFRbteX9wMSERsU0w/XSxv7usxDV4D62rg2E3mzQLuycx9gHuKaai8Mmmf4mcG8IOSatTAuJq3tjXA5Zk5sfi5A6B4JdZUYHyxzT8Vr87S0PEGcFFmjgMOAc4v2tXzu/H01Nbg+d2I1gIfycwDgYnAsRFxCHAZlfZ+D/AycE6x/jnAy8X8y4v1+sTgNYAy8z+A5ZvMPgWYW3yeC5zaZf5Ps+I/gVERsXsphept66Gte3IK8LPMXJuZvwOepvLqLA0RmbkkMx8qPr8GPEHljRue3w1mM23dE8/vIaw4R1cWk1sVPwl8BGgr5m96bm8459uAKRERfTmmwav2xmTmkuLzH4ExxWdfndSYLii6ln6yodsJ27qhFF0L7wcW4Pnd0DZpa/D8bkgRMTwiOoAXgbuAZ4AVmflGsUrXNt3Y3sXyV4Bd+3I8g1eJsnILqbeRNq4fAO+mcrl6CfAPda1GAy4itgduAmZm5qtdl3l+N5Zu2trzu0Fl5puZOZHKm3QOBt5Xy+MZvGrvhQ1dDMXvF4v5Vb06SUNHZr5QnMDrgR/xX90NtnUDiIitqPxHfG1m/ryY7fndgLpra8/vxpeZK4B7gQ9SGR6w4VmnXdt0Y3sXy3cClvXlOAav2rsFmFZ8ngbc3GX+J4q7nw4BXunSZaEhaJMxPKcBG+54vAWYWtwN00JlwPUDZden/ivGcFwJPJGZ3+myyPO7wfTU1p7fjSkiRkfEqOLzSOAoKuP67gVOL1bb9NzecM6fDszPPj4Q1QeoDqCI+BfgSCpvMn8B+DIwD7gBeBfwLHBmZi4vTu4rqNwFswo4OzPb61C2+qGHtj6SSjdEAouBv9nwn21EXAx8ksodUzMz886ya1b/RcSHgF8AvwbWF7O/SGXsj+d3A9lMW38Uz++GExEHUBksP5zKxagbMvOrEbE38DNgF+Bh4OOZuTYimoBrqIz9Ww5Mzczf9umYBi9JkqRy2NUoSZJUEoOXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeElSISKujoiv17sOSY3L4CVJklQSg5ckSVJJDF6ShryIWBwRn42IRRHxp4i4MiLGRMSdEfFaRNwdETsX694YEX+MiFci4j8iYvxm9ntiRHRExIqIuK94r5sk9ZvBS1Kj+GvgKOC9wEnAnVRebjyayr91Fxbr3QnsA/wF8BBwbXc7i4j3Az8B/gbYFfghcEtEbFO7ryCp0Rm8JDWK72fmC5n5HPALYEFmPpyZa4B/Bd4PkJk/yczXMnMtcAlwYETs1M3+ZgA/zMwFmflmZs4F1gKHlPJtJDUkg5ekRvFCl8+ru5nePiKGR8SlEfFMRLwKLC6W79bN/vYCLiq6GVdExApgT+CdA1+6pC3FiHoXIEkl+u/AKcBfUQldOwEvA9HNun8AvpGZ3yitOkkNzytekrYkO1DpLlwGbAt8czPr/gg4NyI+EBXbRcQJEbFDGYVKakwGL0lbkp8CzwLPAY8D/9nTipnZDkwHrqByVexp4KzalyipkUVm1rsGSZKkLYJXvCRJkkpi8JIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKMiQeoLrbbrtlc3NzvcuQJEnq1cKFC1/KzNHdLRsSwau5uZn29vZ6lyFJktSriHi2p2V2NUqSJJXE4CVJklQSg5ckSVJJhsQYL0mSam3dunV0dnayZs2aepeiIaKpqYmxY8ey1VZbVb2NwUuSJKCzs5MddtiB5uZmIqLe5WiQy0yWLVtGZ2cnLS0tVW9nV6MkScCaNWvYddddDV2qSkSw66679vkKqcFLkqSCoUt90Z+/LwYvSZKkkjjGS5IaQPOs2+tdwoBYfOkJ9S5ho4H+M63mu22//fasXLmS9evXM3PmTObPn09E0NTUxA033MDUqVNZu3Yty5cvZ/Xq1eyxxx4AzJs3j+7e8HL88cdz3XXXMWrUqB6PefXVV3P00Ufzzne+c7O1nXXWWZx44omcfvrpHHnkkcyePZvW1taqjlEP8+bN473vfS/jxo3b7Hr9+f5vh8FLkqRB5vrrr+f5559n0aJFDBs2jM7OTrbbbjsWLFgAVMJCe3s7V1xxxWb3c8cdd/R6rKuvvpr999+/1+Dxdo5RD/PmzePEE0+sKni9ne/fV3Y1SpI0yCxZsoTdd9+dYcMq/02PHTuWnXfeuc/7aW5u5qWXXmLx4sXst99+TJ8+nfHjx3P00UezevVq2traaG9v52Mf+xgTJ05k9erVLFy4kCOOOIJJkyZxzDHHsGTJkqqOAfC1r32Nfffdlw996EN89KMfZfbs2QA888wzHHvssUyaNInDDz+c3/zmN0DlKtKFF17IoYceyt57701bW9vG/V522WVMmDCBAw88kFmzZm12P5u67777uOWWW/jsZz/LxIkTeeaZZ+jo6OCQQw7hgAMO4LTTTuPll1/u9vt/9atfZfLkyey///7MmDGDzOzzn/vmGLwkSRpkzjzzTG699VYmTpzIRRddxMMPP/y29/nUU09x/vnn89hjjzFq1ChuuukmTj/9dFpbW7n22mvp6OhgxIgRfPrTn6atrY2FCxfyyU9+kosvvriq/T/44IPcdNNNPPLII9x5551/9o7lGTNm8P3vf5+FCxcye/ZszjvvvI3LlixZwi9/+Utuu+22jQHrzjvv5Oabb2bBggU88sgjfO5zn+t1P10deuihnHzyyXz729+mo6ODd7/73XziE5/gsssuY9GiRUyYMIGvfOUrb/n+I0eO5IILLuDBBx/k0UcfZfXq1dx22239/SPvll2NkiQNMmPHjuXJJ59k/vz5zJ8/nylTpnDjjTcyZcqUfu+zpaWFiRMnAjBp0iQWL178lnWefPJJHn30UY466igA3nzzTXbfffeq9v+rX/2KU045haamJpqamjjppJMAWLlyJffddx9nnHHGxnXXrl278fOpp57KsGHDGDduHC+88AIAd999N2effTbbbrstALvsskuv+9mcV155hRUrVnDEEUcAMG3atD/bT1f33nsv3/rWt1i1ahXLly9n/PjxG7/LQDB4SZI0CG2zzTYcd9xxHHfccYwZM4Z58+a9reC1zTbbbPw8fPhwVq9e/ZZ1MpPx48dz//339/s4m1q/fj2jRo2io6Oj17o2163X234Gwpo1azjvvPNob29nzz335JJLLhnwNxnY1ShJ0iDz0EMP8fzzzwOVwLFo0SL22muvmhxrhx124LXXXgNg3333ZenSpRuD17p163jssceq2s9hhx3Grbfeypo1a1i5cuXGLrodd9yRlpYWbrzxRqASrh555JHN7uuoo47iqquuYtWqVQAsX768z/vp+r122mkndt55Z37xi18AcM0112y8+tV1vQ0ha7fddmPlypV/NuZsoHjFS5KkbtTz0RYvvvgi06dP39iVdvDBB3PBBRfU5FhnnXUW5557LiNHjuT++++nra2NCy+8kFdeeYU33niDmTNnMn78+F73M3nyZE4++WQOOOAAxowZw4QJE9hpp50AuPbaa/nUpz7F17/+ddatW8fUqVM58MADe9zXscceS0dHB62trWy99dYcf/zxfPOb3+zTfqZOncr06dP53ve+R1tbG3PnzuXcc89l1apV7L333lx11VXdfv/p06ez//778453vIPJkyf3409082KgR+vXQmtra3YdpCdJ+nM+x+vte+KJJ9hvv/3qdvxGsHLlSrbffntWrVrFhz/8YebMmcNBBx1U77Jqqru/NxGxMDNbu1vfK16SJGlAzJgxg8cff5w1a9Ywbdq0hg9d/WHwkiRpiPvABz7wljv8rrnmGiZMmFBqHdddd12pxwP4xje+sXHc1wZnnHFG1Y/BKJvBS5KkIW7DE+23RBdffPGgDVnd8a5GSZIKQ2HcswaP/vx9MXhJkgQ0NTWxbNkyw5eqkpksW7aMpqamPm1nV6MkSVSeFt/Z2cnSpUvrXYqGiKamJsaOHdunbQxekiQBW221FS0tLfUuQw3O4CVJGjQa5XlkUN9nkmnwcoyXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUklqGrwi4u8i4rGIeDQi/iUimiKiJSIWRMTTEXF9RGxdyxokSZIGi5oFr4jYA7gQaM3M/YHhwFTgMuDyzHwP8DJwTq1qkCRJGkxq3dU4AhgZESOAbYElwEeAtmL5XODUGtcgSZI0KNQseGXmc8Bs4PdUAtcrwEJgRWa+UazWCexRqxokSZIGk1p2Ne4MnAK0AO8EtgOO7cP2MyKiPSLafW+WJElqBLXsavwr4HeZuTQz1wE/Bw4DRhVdjwBjgee62zgz52Rma2a2jh49uoZlSpIklaOWwev3wCERsW1EBDAFeBy4Fzi9WGcacHMNa5AkSRo0ajnGawGVQfQPAb8ujjUH+DzwmYh4GtgVuLJWNUiSJA0mI3pfpf8y88vAlzeZ/Vvg4FoeV5IkaTDyyfWSJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJXE4CVJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJRlR7wIkqV6aZ91e7xIkbWG84iVJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJXE4CVJklQSg5ckSVJJahq8ImJURLRFxG8i4omI+GBE7BIRd0XEU8XvnWtZgyRJ0mBR6yte3wX+T2a+DzgQeAKYBdyTmfsA9xTTkiRJDa+q4BURE/q644jYCfgwcCVAZr6emSuAU4C5xWpzgVP7um9JkqShqNorXv8UEQ9ExHlFoKpGC7AUuCoiHo6IH0fEdsCYzFxSrPNHYEx3G0fEjIhoj4j2pUuXVnlISZKkwauq4JWZhwMfA/YEFkbEdRFxVC+bjQAOAn6Qme8H/sQm3YqZmUD2cMw5mdmama2jR4+upkxJkqRBreoxXpn5FPAl4PPAEcD3ikHz/08Pm3QCnZm5oJhuoxLEXoiI3QGK3y/2t3hJkqShpNoxXgdExOVUBsd/BDgpM/crPl/e3TaZ+UfgDxGxbzFrCvA4cAswrZg3Dbi5/+VLkiQNHSOqXO/7wI+BL2bm6g0zM/P5iPjSZrb7NHBtRGwN/BY4m0rYuyEizgGeBc7sV+WSJElDTLXB6wRgdWa+CRARw4CmzFyVmdf0tFFmdgCt3Sya0tdCJUmShrpqx3jdDYzsMr1tMU+SJElVqjZ4NWXmyg0Txedta1OSJElSY6o2eP0pIg7aMBERk4DVm1lfkiRJm6h2jNdM4MaIeB4I4B3Af6tVUZIkSY2oquCVmQ9GxPuADY+GeDIz19WuLEmDVfOs2+tdgiQNWdVe8QKYDDQX2xwUEWTmT2tSlSRJUgOqKnhFxDXAu4EO4M1idgIGL0mSpCpVe8WrFRhXvFtRkiRJ/VDtXY2PUhlQL0mSpH6q9orXbsDjEfEAsHbDzMw8uSZVSZIkNaBqg9cltSxCkiRpS1Dt4yT+PSL2AvbJzLsjYltgeG1LkyRJaixVjfGKiOlAG/DDYtYewLwa1SRJktSQqh1cfz5wGPAqQGY+BfxFrYqSJElqRNUGr7WZ+fqGiYgYQeU5XpIkSapStcHr3yPii8DIiDgKuBG4tXZlSZIkNZ5qg9csYCnwa+BvgDuAL9WqKEmSpEZU7V2N64EfFT+SJKkXjfRC+cWXnlDvEhpGte9q/B3djOnKzL0HvCJJkqQG1Zd3NW7QBJwB7DLw5UiSJDWuqsZ4ZeayLj/PZeY/Al53lCRJ6oNquxoP6jI5jMoVsGqvlkmSJInqw9M/dPn8BrAYOHPAq5EkSWpg1d7V+Je1LkSSJKnRVdvV+JnNLc/M7wxMOZIkSY2rL3c1TgZuKaZPAh4AnqpFUZIkSY2o2uA1FjgoM18DiIhLgNsz8+O1KkySJKnRVPvKoDHA612mXy/mSZIkqUrVXvH6KfBARPxrMX0qMLcmFUmSJDWoau9q/EZE3AkcXsw6OzMfrl1ZkiRJjafarkaAbYFXM/O7QGdEtNSoJkmSpIZUVfCKiC8Dnwe+UMzaCvjnKrcdHhEPR8RtxXRLRCyIiKcj4vqI2Lo/hUuSJA011V7xOg04GfgTQGY+D+xQ5bZ/CzzRZfoy4PLMfA/wMnBOlfuRJEka0qoNXq9nZgIJEBHbVbNRRIyl8jLtHxfTAXwEaCtWmUtloL4kSVLDqzZ43RARPwRGRcR04G7gR1Vs94/A54D1xfSuwIrMfKOY7gT2qL5cSZKkoavXuxqLq1TXA+8DXgX2Bf4+M+/qZbsTgRczc2FEHNnXwiJiBjAD4F3veldfN5ckSRp0eg1emZkRcUdmTgA2G7Y2cRhwckQcDzQBOwLfpXLVbERx1Wss8FwPx50DzAFobW3NPhxXkiRpUKq2q/GhiJjclx1n5hcyc2xmNgNTgfmZ+THgXuD0YrVpwM192a8kSdJQVW3w+gDwnxHxTEQsiohfR8Sifh7z88BnIuJpKmO+ruznfiRJkoaUzXY1RsS7MvP3wDFv5yCZ+W/AvxWffwsc/Hb2J0mSNBT1NsZrHnBQZj4bETdl5l+XUJMkSVJD6q2rMbp83ruWhUiSJDW63oJX9vBZkiRJfdRbV+OBEfEqlStfI4vPFNOZmTvWtDpJkqQGstnglZnDyypEkiSp0VX7OAlJkiS9TQYvSZKkkhi8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSpJb0+ulzQAmmfdXu8SJEmDgFe8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSqJwUuSJKkkBi9JkqSSGLwkSZJKYvCSJEkqicFLkiSpJAYvSZKkkhi8JEmSSmLwkiRJKonBS5IkqSQGL0mSpJIYvCRJkkpi8JIkSSpJzYJXROwZEfdGxOMR8VhE/G0xf5eIuCsinip+71yrGiRJkgaTETXc9xvARZn5UETsACyMiLuAs4B7MvPSiJgFzAI+X8M6NIQ1z7q93iVIkjRganbFKzOXZOZDxefXgCeAPYBTgLnFanOBU2tVgyRJ0mBSyhiviGgG3g8sAMZk5pJi0R+BMT1sMyMi2iOifenSpWWUKUmSVFM1D14RsT1wEzAzM1/tuiwzE8jutsvMOZnZmpmto0ePrnWZkiRJNVfT4BURW1EJXddm5s+L2S9ExO7F8t2BF2tZgyRJ0mBRy7saA7gSeCIzv9Nl0S3AtOLzNODmWtUgSZI0mNTyrsbDgP8B/DoiOop5XwQuBW6IiHOAZ4Eza1iDJEnSoFGz4JWZvwSih8VTanVcSZKkwcon10uSJJXE4CVJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJVkRL0LkCRJg1vzrNvrXcKAWHzpCfUuwStekiRJZTF4SZIklcTgJUmSVBKDlyRJUkkMXpIkSSUxeEmSJJXE4CVJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEt/V2IAa5Z1akiQ1Gq94SZIklcQrXgWvEkmSpFrzipckSVJJDF6SJEklMXhJkiSVpC7BKyKOjYgnI+LpiJhVjxokSZLKVnrwiojhwP8CjgPGAR+NiHFl1yFJklS2elzxOhh4OjN/m5mvAz8DTqlDHZIkSaWqR/DaA/hDl+nOYp4kSVJDG7TP8YqIGcCMYnJlRDxZz3q2cLsBL9W7CPWb7Td02XZDl203CMVlVa02EG23V08L6hG8ngP27DI9tpj3ZzJzDjCnrKLUs4hoz8zWeteh/rH9hi7bbuiy7YauWrddPboaHwT2iYiWiNgamArcUoc6JEmSSlX6Fa/MfCMiLgD+LzAc+ElmPlZ2HZIkSWWryxivzLwDuKMex1a/2OU7tNl+Q5dtN3TZdkNXTdsuMrOW+5ckSVLBVwZJkiSVxOAlIuInEfFiRDzaZd4uEXFXRDxV/N65mB8R8b3idU+LIuKg+lWuHtrukoh4LiI6ip/juyz7QtF2T0bEMfWpWgARsWdE3BsRj0fEYxHxt8V8z71BbjNt57k3BEREU0Q8EBGPFO33lWJ+S0QsKNrp+uIGQCJim2L66WJ589s5vsFLAFcDx24ybxZwT2buA9xTTEPlVU/7FD8zgB+UVKO6dzVvbTuAyzNzYvFzB0Dxaq6pwPhim38qXuGl+ngDuCgzxwGHAOcXbeS5N/j11HbguTcUrAU+kpkHAhOBYyPiEOAyKu33HuBl4Jxi/XOAl4v5lxfr9ZvBS2TmfwDLN5l9CjC3+DwXOLXL/J9mxX8CoyJi91IK1Vv00HY9OQX4WWauzczfAU9TeYWX6iAzl2TmQ8Xn14AnqLzFw3NvkNtM2/XEc28QKc6hlcXkVsVPAh8B2or5m557G87JNmBKRER/j2/wUk/GZOaS4vMfgTHFZ1/5NDRcUHRH/WRDVxW23aBVdF28H1iA596QsknbgefekBARwyOiA3gRuAt4BliRmW8Uq3Rto43tVyx/Bdi1v8c2eKlXWbn11dtfh44fAO+mcgl9CfAPda1GmxUR2wM3ATMz89Wuyzz3Brdu2s5zb4jIzDczcyKVt+ccDLyvrGMbvNSTFzZ0YxS/XyzmV/XKJ9VPZr5Q/KOyHvgR/9WlYdsNMhGxFZX/uK/NzJ8Xsz33hoDu2s5zb+jJzBXAvcAHqXTfb3i+adc22th+xfKdgGX9PabBSz25BZhWfJ4G3Nxl/ieKO6wOAV7p0i2iQWCTcT+nARvueLwFmFrcodNCZZD2A2XXp4pijMiVwBOZ+Z0uizz3Brme2s5zb2iIiNERMar4PBI4iso4vXuB04vVNj33NpyTpwPz8208BNUHqIqI+BfgSCpvZH8B+DIwD7gBeBfwLHBmZi4v/sG5gsqdOauAszOzvQ5lix7b7kgqXR0JLAb+ZsN/0BFxMfBJKndlzczMO8uuWRUR8SHgF8CvgfXF7C9SGSvkuTeIbabtPorn3qAXEQdQGSw/nMoFqBsy86sRsTfwM2AX4GHg45m5NiKagGuojOVbDkzNzN/2+/gGL0mSpHLY1ShJklQSg5ckSVJJDF6SJEklMXhJkiSVxOAlSZJUEoOXJElSSQxekiRJJTF4SZIkleT/B0evDRtA0dQjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "subsampled_demographics.plot.hist(column=[\"IST_intelligence_total\"], by=\"sex\", figsize=(10, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e84fc06",
   "metadata": {},
   "source": [
    "The descriptive statistics and the histogram show that the measure of intelligence is roughly normally distributed between values of 100 and 300 with a mean of 200.4 and a standard deviation of 40.77."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f3571d",
   "metadata": {},
   "source": [
    "Also we will check if there are any NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f39caac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled_demographics[\"IST_intelligence_total\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73dcb9",
   "metadata": {},
   "source": [
    "We can see that there are two instances with missing values which we will have to exclude from the analysis. Let's have a look at the 'NaN' samples.\n",
    "\n",
    "We can use the return value of the **isna()** method to index the original dataframe to see which samples have the 'NaN' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab9d6d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>handedness</th>\n",
       "      <th>BMI</th>\n",
       "      <th>education_level</th>\n",
       "      <th>background_SES</th>\n",
       "      <th>IST_fluid</th>\n",
       "      <th>IST_memory</th>\n",
       "      <th>IST_crystallised</th>\n",
       "      <th>IST_intelligence_total</th>\n",
       "      <th>...</th>\n",
       "      <th>sexual_attraction_M</th>\n",
       "      <th>sexual_attraction_F</th>\n",
       "      <th>gender_identity_M</th>\n",
       "      <th>gender_identity_F</th>\n",
       "      <th>religious_upbringing</th>\n",
       "      <th>religious_now</th>\n",
       "      <th>religious_importance</th>\n",
       "      <th>DWI_TR_run1</th>\n",
       "      <th>DWI_TR_run2</th>\n",
       "      <th>DWI_TR_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0127</th>\n",
       "      <td>25.25</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>24</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0390</th>\n",
       "      <td>20.50</td>\n",
       "      <td>female</td>\n",
       "      <td>left</td>\n",
       "      <td>22</td>\n",
       "      <td>medium</td>\n",
       "      <td>3.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.313</td>\n",
       "      <td>6.313</td>\n",
       "      <td>6.313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age     sex handedness  BMI education_level  background_SES  \\\n",
       "sub-0127  25.25  female      right   24          medium             2.0   \n",
       "sub-0390  20.50  female       left   22          medium             3.5   \n",
       "\n",
       "          IST_fluid  IST_memory  IST_crystallised  IST_intelligence_total  \\\n",
       "sub-0127        NaN         NaN               NaN                     NaN   \n",
       "sub-0390        NaN         NaN               NaN                     NaN   \n",
       "\n",
       "          ...  sexual_attraction_M  sexual_attraction_F  gender_identity_M  \\\n",
       "sub-0127  ...                  NaN                  NaN                NaN   \n",
       "sub-0390  ...                  NaN                  NaN                NaN   \n",
       "\n",
       "          gender_identity_F  religious_upbringing  religious_now  \\\n",
       "sub-0127                NaN                   yes            yes   \n",
       "sub-0390                NaN                    no             no   \n",
       "\n",
       "          religious_importance  DWI_TR_run1  DWI_TR_run2  DWI_TR_run3  \n",
       "sub-0127                   5.0        6.312        6.312        6.312  \n",
       "sub-0390                   NaN        6.313        6.313        6.313  \n",
       "\n",
       "[2 rows x 30 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled_demographics.loc[subsampled_demographics[\"IST_intelligence_total\"].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ad75d",
   "metadata": {},
   "source": [
    "Likewise, we can use the **~** operator to negate or invert the boolean values of the **isna()** array to index the subjects which **do not** have 'NaN' values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c6491c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>handedness</th>\n",
       "      <th>BMI</th>\n",
       "      <th>education_level</th>\n",
       "      <th>background_SES</th>\n",
       "      <th>IST_fluid</th>\n",
       "      <th>IST_memory</th>\n",
       "      <th>IST_crystallised</th>\n",
       "      <th>IST_intelligence_total</th>\n",
       "      <th>...</th>\n",
       "      <th>sexual_attraction_M</th>\n",
       "      <th>sexual_attraction_F</th>\n",
       "      <th>gender_identity_M</th>\n",
       "      <th>gender_identity_F</th>\n",
       "      <th>religious_upbringing</th>\n",
       "      <th>religious_now</th>\n",
       "      <th>religious_importance</th>\n",
       "      <th>DWI_TR_run1</th>\n",
       "      <th>DWI_TR_run2</th>\n",
       "      <th>DWI_TR_run3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>22.00</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>23</td>\n",
       "      <td>medium</td>\n",
       "      <td>2.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>159.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>21.75</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>20</td>\n",
       "      <td>medium</td>\n",
       "      <td>5.5</td>\n",
       "      <td>97.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>25.25</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>31</td>\n",
       "      <td>high</td>\n",
       "      <td>3.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>227.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "      <td>6.312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>22.50</td>\n",
       "      <td>female</td>\n",
       "      <td>right</td>\n",
       "      <td>20</td>\n",
       "      <td>high</td>\n",
       "      <td>5.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>270.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>22.25</td>\n",
       "      <td>male</td>\n",
       "      <td>right</td>\n",
       "      <td>23</td>\n",
       "      <td>high</td>\n",
       "      <td>4.5</td>\n",
       "      <td>112.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>212.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "      <td>6.311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            age     sex handedness  BMI education_level  background_SES  \\\n",
       "sub-0001  22.00  female      right   23          medium             2.0   \n",
       "sub-0002  21.75  female      right   20          medium             5.5   \n",
       "sub-0003  25.25  female      right   31            high             3.0   \n",
       "sub-0004  22.50  female      right   20            high             5.0   \n",
       "sub-0005  22.25    male      right   23            high             4.5   \n",
       "\n",
       "          IST_fluid  IST_memory  IST_crystallised  IST_intelligence_total  \\\n",
       "sub-0001       77.0        49.0              33.0                   159.0   \n",
       "sub-0002       97.0        63.0              39.0                   199.0   \n",
       "sub-0003      122.0        67.0              38.0                   227.0   \n",
       "sub-0004      149.0        69.0              52.0                   270.0   \n",
       "sub-0005      112.0        57.0              43.0                   212.0   \n",
       "\n",
       "          ...  sexual_attraction_M  sexual_attraction_F  gender_identity_M  \\\n",
       "sub-0001  ...                  7.0                  1.0                1.0   \n",
       "sub-0002  ...                  7.0                  1.0                2.0   \n",
       "sub-0003  ...                  6.0                  3.0                1.0   \n",
       "sub-0004  ...                  6.0                  2.0                1.0   \n",
       "sub-0005  ...                  1.0                  7.0                6.0   \n",
       "\n",
       "          gender_identity_F  religious_upbringing  religious_now  \\\n",
       "sub-0001                7.0                    no            yes   \n",
       "sub-0002                7.0                    no             no   \n",
       "sub-0003                6.0                    no             no   \n",
       "sub-0004                7.0                   yes             no   \n",
       "sub-0005                1.0                    no             no   \n",
       "\n",
       "          religious_importance  DWI_TR_run1  DWI_TR_run2  DWI_TR_run3  \n",
       "sub-0001                   2.0        6.312        6.312        6.312  \n",
       "sub-0002                   NaN          NaN        6.311        6.311  \n",
       "sub-0003                   NaN        6.312        6.312        6.312  \n",
       "sub-0004                   NaN        6.311        6.311        6.311  \n",
       "sub-0005                   NaN        6.311        6.311        6.311  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_target_nans = subsampled_demographics[~subsampled_demographics[\"IST_intelligence_total\"].isna()].copy()\n",
    "exclude_target_nans.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dc4423",
   "metadata": {},
   "source": [
    "The **exclude_target_nans** dataframe now only contains data that actually has data for our target.\n",
    "\n",
    "We can convince ourselves that this worked correctly by double checking the 'NaN' count. These types of sanity checks (double checking that your code worked even if you think it is obvious can be extremely useful at catching problems early on):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "710146f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude_target_nans[\"IST_intelligence_total\"].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a827f9",
   "metadata": {},
   "source": [
    "As expected this returns 0, so we are all good. Now we need to just select the same subjects for the connectomes and make sure that rows are in the same order for both features as well as the target. We can do this by indexing the connectomes using the index from our target data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80d63056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows (subjects) x Columns (features)\n",
      "(875, 8646)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LH_VisCent_ExStr_2~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisCent_Striate_1~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisCent_Striate_1~LH_VisCent_ExStr_2</th>\n",
       "      <th>LH_VisCent_ExStr_3~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisCent_ExStr_3~LH_VisCent_ExStr_2</th>\n",
       "      <th>LH_VisCent_ExStr_3~LH_VisCent_Striate_1</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_1</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_2</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_Striate_1</th>\n",
       "      <th>LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_3</th>\n",
       "      <th>...</th>\n",
       "      <th>pCAU-lh~THA-VP-lh</th>\n",
       "      <th>pCAU-lh~THA-VA-lh</th>\n",
       "      <th>pCAU-lh~THA-DA-lh</th>\n",
       "      <th>pCAU-lh~NAc-shell-lh</th>\n",
       "      <th>pCAU-lh~NAc-core-lh</th>\n",
       "      <th>pCAU-lh~pGP-lh</th>\n",
       "      <th>pCAU-lh~aGP-lh</th>\n",
       "      <th>pCAU-lh~aPUT-lh</th>\n",
       "      <th>pCAU-lh~pPUT-lh</th>\n",
       "      <th>pCAU-lh~aCAU-lh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sub-0001</th>\n",
       "      <td>0.533892</td>\n",
       "      <td>0.638701</td>\n",
       "      <td>0.560450</td>\n",
       "      <td>0.789158</td>\n",
       "      <td>0.466875</td>\n",
       "      <td>0.539399</td>\n",
       "      <td>0.677232</td>\n",
       "      <td>0.366597</td>\n",
       "      <td>0.361958</td>\n",
       "      <td>0.564919</td>\n",
       "      <td>...</td>\n",
       "      <td>0.072425</td>\n",
       "      <td>0.366347</td>\n",
       "      <td>0.327794</td>\n",
       "      <td>-0.069095</td>\n",
       "      <td>0.468919</td>\n",
       "      <td>-0.071477</td>\n",
       "      <td>0.040889</td>\n",
       "      <td>0.212324</td>\n",
       "      <td>0.173870</td>\n",
       "      <td>0.664122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0002</th>\n",
       "      <td>0.641335</td>\n",
       "      <td>0.663375</td>\n",
       "      <td>0.545326</td>\n",
       "      <td>0.629451</td>\n",
       "      <td>0.187963</td>\n",
       "      <td>0.390782</td>\n",
       "      <td>0.670098</td>\n",
       "      <td>0.116997</td>\n",
       "      <td>0.397939</td>\n",
       "      <td>0.742349</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048029</td>\n",
       "      <td>0.294397</td>\n",
       "      <td>0.247182</td>\n",
       "      <td>0.050190</td>\n",
       "      <td>0.069237</td>\n",
       "      <td>-0.133454</td>\n",
       "      <td>0.011535</td>\n",
       "      <td>0.265424</td>\n",
       "      <td>0.147957</td>\n",
       "      <td>0.587463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0003</th>\n",
       "      <td>0.657928</td>\n",
       "      <td>0.742613</td>\n",
       "      <td>0.453348</td>\n",
       "      <td>0.860049</td>\n",
       "      <td>0.523853</td>\n",
       "      <td>0.591320</td>\n",
       "      <td>0.770457</td>\n",
       "      <td>0.418997</td>\n",
       "      <td>0.645827</td>\n",
       "      <td>0.725303</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145307</td>\n",
       "      <td>0.240758</td>\n",
       "      <td>-0.014584</td>\n",
       "      <td>0.125811</td>\n",
       "      <td>0.397896</td>\n",
       "      <td>0.112837</td>\n",
       "      <td>0.292399</td>\n",
       "      <td>0.473047</td>\n",
       "      <td>0.224996</td>\n",
       "      <td>0.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0004</th>\n",
       "      <td>0.788957</td>\n",
       "      <td>0.694655</td>\n",
       "      <td>0.660015</td>\n",
       "      <td>0.732351</td>\n",
       "      <td>0.517771</td>\n",
       "      <td>0.387978</td>\n",
       "      <td>0.620954</td>\n",
       "      <td>0.283812</td>\n",
       "      <td>0.426040</td>\n",
       "      <td>0.622988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288743</td>\n",
       "      <td>0.276673</td>\n",
       "      <td>0.355181</td>\n",
       "      <td>0.250076</td>\n",
       "      <td>0.216825</td>\n",
       "      <td>-0.064241</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>0.379170</td>\n",
       "      <td>0.226584</td>\n",
       "      <td>0.388584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0005</th>\n",
       "      <td>0.594372</td>\n",
       "      <td>0.761742</td>\n",
       "      <td>0.667648</td>\n",
       "      <td>0.791777</td>\n",
       "      <td>0.467393</td>\n",
       "      <td>0.515712</td>\n",
       "      <td>0.285025</td>\n",
       "      <td>-0.306764</td>\n",
       "      <td>0.179985</td>\n",
       "      <td>0.250133</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009222</td>\n",
       "      <td>0.393067</td>\n",
       "      <td>0.032954</td>\n",
       "      <td>0.220143</td>\n",
       "      <td>0.289458</td>\n",
       "      <td>0.012017</td>\n",
       "      <td>0.073975</td>\n",
       "      <td>0.515776</td>\n",
       "      <td>0.426828</td>\n",
       "      <td>0.701249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0923</th>\n",
       "      <td>0.580657</td>\n",
       "      <td>0.581537</td>\n",
       "      <td>0.456695</td>\n",
       "      <td>0.852308</td>\n",
       "      <td>0.345382</td>\n",
       "      <td>0.325712</td>\n",
       "      <td>0.656729</td>\n",
       "      <td>0.186045</td>\n",
       "      <td>0.466766</td>\n",
       "      <td>0.692227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.356343</td>\n",
       "      <td>0.386007</td>\n",
       "      <td>0.461522</td>\n",
       "      <td>-0.158422</td>\n",
       "      <td>0.364220</td>\n",
       "      <td>-0.159627</td>\n",
       "      <td>-0.157892</td>\n",
       "      <td>0.505149</td>\n",
       "      <td>0.343721</td>\n",
       "      <td>0.622666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0924</th>\n",
       "      <td>0.772896</td>\n",
       "      <td>0.576773</td>\n",
       "      <td>0.382912</td>\n",
       "      <td>0.857393</td>\n",
       "      <td>0.595229</td>\n",
       "      <td>0.373345</td>\n",
       "      <td>0.384935</td>\n",
       "      <td>0.119016</td>\n",
       "      <td>0.407748</td>\n",
       "      <td>0.466081</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001937</td>\n",
       "      <td>0.384416</td>\n",
       "      <td>0.317110</td>\n",
       "      <td>-0.004191</td>\n",
       "      <td>0.178207</td>\n",
       "      <td>-0.105206</td>\n",
       "      <td>0.375226</td>\n",
       "      <td>0.427756</td>\n",
       "      <td>0.441114</td>\n",
       "      <td>0.711478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0925</th>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.674962</td>\n",
       "      <td>0.643015</td>\n",
       "      <td>0.708971</td>\n",
       "      <td>0.593741</td>\n",
       "      <td>0.415912</td>\n",
       "      <td>0.240876</td>\n",
       "      <td>0.030883</td>\n",
       "      <td>0.435190</td>\n",
       "      <td>0.354092</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123305</td>\n",
       "      <td>0.391051</td>\n",
       "      <td>0.101072</td>\n",
       "      <td>0.482087</td>\n",
       "      <td>0.363243</td>\n",
       "      <td>0.259769</td>\n",
       "      <td>0.127300</td>\n",
       "      <td>0.283468</td>\n",
       "      <td>0.446741</td>\n",
       "      <td>0.472104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0926</th>\n",
       "      <td>0.699854</td>\n",
       "      <td>0.751275</td>\n",
       "      <td>0.615963</td>\n",
       "      <td>0.834274</td>\n",
       "      <td>0.421496</td>\n",
       "      <td>0.565254</td>\n",
       "      <td>0.829891</td>\n",
       "      <td>0.371156</td>\n",
       "      <td>0.578320</td>\n",
       "      <td>0.874583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.179298</td>\n",
       "      <td>0.550858</td>\n",
       "      <td>0.605740</td>\n",
       "      <td>0.030332</td>\n",
       "      <td>0.411371</td>\n",
       "      <td>0.123619</td>\n",
       "      <td>0.216562</td>\n",
       "      <td>0.465273</td>\n",
       "      <td>0.301320</td>\n",
       "      <td>0.666110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sub-0927</th>\n",
       "      <td>0.704007</td>\n",
       "      <td>0.587890</td>\n",
       "      <td>0.530191</td>\n",
       "      <td>0.792209</td>\n",
       "      <td>0.678987</td>\n",
       "      <td>0.677505</td>\n",
       "      <td>0.632512</td>\n",
       "      <td>0.392710</td>\n",
       "      <td>0.567254</td>\n",
       "      <td>0.640225</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079181</td>\n",
       "      <td>0.332753</td>\n",
       "      <td>0.391636</td>\n",
       "      <td>-0.162361</td>\n",
       "      <td>0.369515</td>\n",
       "      <td>-0.013173</td>\n",
       "      <td>0.136597</td>\n",
       "      <td>0.581985</td>\n",
       "      <td>0.545981</td>\n",
       "      <td>0.615678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>875 rows Ã— 8646 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          LH_VisCent_ExStr_2~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                               0.533892   \n",
       "sub-0002                               0.641335   \n",
       "sub-0003                               0.657928   \n",
       "sub-0004                               0.788957   \n",
       "sub-0005                               0.594372   \n",
       "...                                         ...   \n",
       "sub-0923                               0.580657   \n",
       "sub-0924                               0.772896   \n",
       "sub-0925                               0.836000   \n",
       "sub-0926                               0.699854   \n",
       "sub-0927                               0.704007   \n",
       "\n",
       "          LH_VisCent_Striate_1~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                                 0.638701   \n",
       "sub-0002                                 0.663375   \n",
       "sub-0003                                 0.742613   \n",
       "sub-0004                                 0.694655   \n",
       "sub-0005                                 0.761742   \n",
       "...                                           ...   \n",
       "sub-0923                                 0.581537   \n",
       "sub-0924                                 0.576773   \n",
       "sub-0925                                 0.674962   \n",
       "sub-0926                                 0.751275   \n",
       "sub-0927                                 0.587890   \n",
       "\n",
       "          LH_VisCent_Striate_1~LH_VisCent_ExStr_2  \\\n",
       "sub-0001                                 0.560450   \n",
       "sub-0002                                 0.545326   \n",
       "sub-0003                                 0.453348   \n",
       "sub-0004                                 0.660015   \n",
       "sub-0005                                 0.667648   \n",
       "...                                           ...   \n",
       "sub-0923                                 0.456695   \n",
       "sub-0924                                 0.382912   \n",
       "sub-0925                                 0.643015   \n",
       "sub-0926                                 0.615963   \n",
       "sub-0927                                 0.530191   \n",
       "\n",
       "          LH_VisCent_ExStr_3~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                               0.789158   \n",
       "sub-0002                               0.629451   \n",
       "sub-0003                               0.860049   \n",
       "sub-0004                               0.732351   \n",
       "sub-0005                               0.791777   \n",
       "...                                         ...   \n",
       "sub-0923                               0.852308   \n",
       "sub-0924                               0.857393   \n",
       "sub-0925                               0.708971   \n",
       "sub-0926                               0.834274   \n",
       "sub-0927                               0.792209   \n",
       "\n",
       "          LH_VisCent_ExStr_3~LH_VisCent_ExStr_2  \\\n",
       "sub-0001                               0.466875   \n",
       "sub-0002                               0.187963   \n",
       "sub-0003                               0.523853   \n",
       "sub-0004                               0.517771   \n",
       "sub-0005                               0.467393   \n",
       "...                                         ...   \n",
       "sub-0923                               0.345382   \n",
       "sub-0924                               0.595229   \n",
       "sub-0925                               0.593741   \n",
       "sub-0926                               0.421496   \n",
       "sub-0927                               0.678987   \n",
       "\n",
       "          LH_VisCent_ExStr_3~LH_VisCent_Striate_1  \\\n",
       "sub-0001                                 0.539399   \n",
       "sub-0002                                 0.390782   \n",
       "sub-0003                                 0.591320   \n",
       "sub-0004                                 0.387978   \n",
       "sub-0005                                 0.515712   \n",
       "...                                           ...   \n",
       "sub-0923                                 0.325712   \n",
       "sub-0924                                 0.373345   \n",
       "sub-0925                                 0.415912   \n",
       "sub-0926                                 0.565254   \n",
       "sub-0927                                 0.677505   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_1  \\\n",
       "sub-0001                                  0.677232   \n",
       "sub-0002                                  0.670098   \n",
       "sub-0003                                  0.770457   \n",
       "sub-0004                                  0.620954   \n",
       "sub-0005                                  0.285025   \n",
       "...                                            ...   \n",
       "sub-0923                                  0.656729   \n",
       "sub-0924                                  0.384935   \n",
       "sub-0925                                  0.240876   \n",
       "sub-0926                                  0.829891   \n",
       "sub-0927                                  0.632512   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_2  \\\n",
       "sub-0001                                  0.366597   \n",
       "sub-0002                                  0.116997   \n",
       "sub-0003                                  0.418997   \n",
       "sub-0004                                  0.283812   \n",
       "sub-0005                                 -0.306764   \n",
       "...                                            ...   \n",
       "sub-0923                                  0.186045   \n",
       "sub-0924                                  0.119016   \n",
       "sub-0925                                  0.030883   \n",
       "sub-0926                                  0.371156   \n",
       "sub-0927                                  0.392710   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_Striate_1  \\\n",
       "sub-0001                                    0.361958   \n",
       "sub-0002                                    0.397939   \n",
       "sub-0003                                    0.645827   \n",
       "sub-0004                                    0.426040   \n",
       "sub-0005                                    0.179985   \n",
       "...                                              ...   \n",
       "sub-0923                                    0.466766   \n",
       "sub-0924                                    0.407748   \n",
       "sub-0925                                    0.435190   \n",
       "sub-0926                                    0.578320   \n",
       "sub-0927                                    0.567254   \n",
       "\n",
       "          LH_VisPeri_ExStrInf_1~LH_VisCent_ExStr_3  ...  pCAU-lh~THA-VP-lh  \\\n",
       "sub-0001                                  0.564919  ...           0.072425   \n",
       "sub-0002                                  0.742349  ...          -0.048029   \n",
       "sub-0003                                  0.725303  ...          -0.145307   \n",
       "sub-0004                                  0.622988  ...           0.288743   \n",
       "sub-0005                                  0.250133  ...          -0.009222   \n",
       "...                                            ...  ...                ...   \n",
       "sub-0923                                  0.692227  ...           0.356343   \n",
       "sub-0924                                  0.466081  ...          -0.001937   \n",
       "sub-0925                                  0.354092  ...          -0.123305   \n",
       "sub-0926                                  0.874583  ...           0.179298   \n",
       "sub-0927                                  0.640225  ...          -0.079181   \n",
       "\n",
       "          pCAU-lh~THA-VA-lh  pCAU-lh~THA-DA-lh  pCAU-lh~NAc-shell-lh  \\\n",
       "sub-0001           0.366347           0.327794             -0.069095   \n",
       "sub-0002           0.294397           0.247182              0.050190   \n",
       "sub-0003           0.240758          -0.014584              0.125811   \n",
       "sub-0004           0.276673           0.355181              0.250076   \n",
       "sub-0005           0.393067           0.032954              0.220143   \n",
       "...                     ...                ...                   ...   \n",
       "sub-0923           0.386007           0.461522             -0.158422   \n",
       "sub-0924           0.384416           0.317110             -0.004191   \n",
       "sub-0925           0.391051           0.101072              0.482087   \n",
       "sub-0926           0.550858           0.605740              0.030332   \n",
       "sub-0927           0.332753           0.391636             -0.162361   \n",
       "\n",
       "          pCAU-lh~NAc-core-lh  pCAU-lh~pGP-lh  pCAU-lh~aGP-lh  \\\n",
       "sub-0001             0.468919       -0.071477        0.040889   \n",
       "sub-0002             0.069237       -0.133454        0.011535   \n",
       "sub-0003             0.397896        0.112837        0.292399   \n",
       "sub-0004             0.216825       -0.064241        0.045752   \n",
       "sub-0005             0.289458        0.012017        0.073975   \n",
       "...                       ...             ...             ...   \n",
       "sub-0923             0.364220       -0.159627       -0.157892   \n",
       "sub-0924             0.178207       -0.105206        0.375226   \n",
       "sub-0925             0.363243        0.259769        0.127300   \n",
       "sub-0926             0.411371        0.123619        0.216562   \n",
       "sub-0927             0.369515       -0.013173        0.136597   \n",
       "\n",
       "          pCAU-lh~aPUT-lh  pCAU-lh~pPUT-lh  pCAU-lh~aCAU-lh  \n",
       "sub-0001         0.212324         0.173870         0.664122  \n",
       "sub-0002         0.265424         0.147957         0.587463  \n",
       "sub-0003         0.473047         0.224996         0.711400  \n",
       "sub-0004         0.379170         0.226584         0.388584  \n",
       "sub-0005         0.515776         0.426828         0.701249  \n",
       "...                   ...              ...              ...  \n",
       "sub-0923         0.505149         0.343721         0.622666  \n",
       "sub-0924         0.427756         0.441114         0.711478  \n",
       "sub-0925         0.283468         0.446741         0.472104  \n",
       "sub-0926         0.465273         0.301320         0.666110  \n",
       "sub-0927         0.581985         0.545981         0.615678  \n",
       "\n",
       "[875 rows x 8646 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsampled_connectomes = connectomes.loc[exclude_target_nans.index]\n",
    "print(\"Rows (subjects) x Columns (features)\")\n",
    "print(subsampled_connectomes.shape)\n",
    "subsampled_connectomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcb1132",
   "metadata": {},
   "source": [
    "As a last step before we start training models, let's turn the data into a format that sklearn understands: the **numpy array**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37c50c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array(subsampled_connectomes)\n",
    "y = np.array(exclude_target_nans[\"IST_intelligence_total\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7913d282",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "Let us first split the data so we have one hold-out validation set that will be left untouched for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6a127d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_model_selection, X_holdout, y_model_selection, y_holdout = train_test_split(X, y, random_state=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05b3ef4",
   "metadata": {},
   "source": [
    "### Fitting a bunch of models\n",
    "\n",
    "Every problem, every classification or regression task is different, and therefore requires a different model. In other words, which model works best depends on the underlying processes that generate the distribution of our X and on the \"true\" function that maps X to y. In other words, we cannot really know which model will work best before we try it out. Let us test out a few popular options therefore starting with a **Ridge Regression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e35e3aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd44a9",
   "metadata": {},
   "source": [
    "If we check out the **sklearn documentation**, we see that the **Ridge Regression estimator** has a parameter called **alpha**, which can be set to a positive floating point value. This is a **hyperparameter**, *that must be set by the user and cannot be fitted based on the data*. How can we know which value is the best one? First, run the code in the next cell to see the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7078beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "?Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7c2227",
   "metadata": {},
   "source": [
    "A simple approach would be to try out a number of different **candidate values** for **alpha** and see how well they compare to each other. That is, we could define a **grid** of candidate values, and **search** the candidate that gives us the best outcome. In order to make sure the estimate of the goodness of the outcome (i.e. our estimate of the error) is not dependent on a specific train-test split, we want to perform a cross-validation for every candidate value.\n",
    "\n",
    "Scikit-learn allows us to perform **model selection using a cross-validated gridsearch** using the **GridSearchCV** object. Let's import it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa131bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0d860",
   "metadata": {},
   "source": [
    "Let's check out the documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "249bbdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "?GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcd168e",
   "metadata": {},
   "source": [
    "We can see many parameters, but there are 4 which we care about predominantly:\n",
    "\n",
    "1. **\"estimator\"** -> our sklearn estimator object (i.e. the model class)\n",
    "2. **\"param_grid\"** -> the grid of hyperparameters to search\n",
    "3. **\"scoring\"** -> which scoring metric to use\n",
    "4. **\"cv\"** -> the cross-validation scheme to use\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92034c2b",
   "metadata": {},
   "source": [
    "For the moment, let's go with the example of the ridge regressor, for which we want to tune the alpha value. We can define the estimator as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3626afb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b10401",
   "metadata": {},
   "source": [
    "The **param_grid** parameter typically is handed over as a **dictionary** in which the **keys** consist of the names of the parameters that are to be set for the estimator, and the **values** of the dictionary each yield an **iterable** (for example a **list**) of **possible candidate values** for each parameter.\n",
    "\n",
    "For our Ridge model, we can define a very simple grid with candidate values for the alpha parameter as follows. Note, that this is only an example grid and should not serve you as a future reference as a grid for a Ridge regressor (that is, don't just blindly copy this grid if you want to use a ridge regressor in your work). The values that you choose for your grid can highly depend on your goals and the problem at hand, but it can also help to search the literature that has used the models you want to use and see how they may have defined a grid to search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "366e2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ridge = {\n",
    "    \"alpha\": [0.001, 0.01, 0.1, 1, 2, 10, 50, 100, 200, 300, 500, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dc8ed9",
   "metadata": {},
   "source": [
    "The scoring parameter defines which scoring metric we care about, i.e. which scoring metric should be optimized by the grid search. It can be a string if the metric is already in-built in sklearn. For simplicity we will use [\"r2\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html) here, otherwise known as coefficient of determination.\n",
    "\n",
    "To see what other metrics are available check out this: https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd81b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoring = \"r2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32abbebf",
   "metadata": {},
   "source": [
    "Lastly, the \"cv\" parameter can be any scikit-learn compatible cross-validation scheme. Here we will use a simple 5-fold cross-validation. We should also make sure that the KFold cv shuffles the data, but with a specific random state, so that the results are reproducible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f44fed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8535c5f2",
   "metadata": {},
   "source": [
    "We can then initialise the GridSearchCV object, and fit it like any other scikit-learn estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4046fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=KFold(n_splits=5, random_state=100, shuffle=True),\n",
       "             estimator=Ridge(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 2, 10, 50, 100, 200,\n",
       "                                   300, 500, 1000]},\n",
       "             scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=KFold(n_splits=5, random_state=100, shuffle=True),\n",
       "             estimator=Ridge(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.001, 0.01, 0.1, 1, 2, 10, 50, 100, 200,\n",
       "                                   300, 500, 1000]},\n",
       "             scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=KFold(n_splits=5, random_state=100, shuffle=True),\n",
       "             estimator=Ridge(),\n",
       "             param_grid={'alpha': [0.001, 0.01, 0.1, 1, 2, 10, 50, 100, 200,\n",
       "                                   300, 500, 1000]},\n",
       "             scoring='r2')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearchcv = GridSearchCV(\n",
    "    estimator=ridge,\n",
    "    param_grid=param_grid_ridge,\n",
    "    scoring=scoring,\n",
    "    cv=kfold\n",
    ")\n",
    "\n",
    "gridsearchcv.fit(X_model_selection, y_model_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8580e1fa",
   "metadata": {},
   "source": [
    "The GridSearchCV has an **attribute** called **cv_results_** which we can access as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25e704fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.14264998, 0.14849219, 0.14051337, 0.15535626, 0.1602098 ,\n",
       "        0.09116535, 0.10856662, 0.14306507, 0.16946912, 0.13145599,\n",
       "        0.14638906, 0.13995094]),\n",
       " 'std_fit_time': array([0.0379892 , 0.03858072, 0.04071749, 0.02528383, 0.03628036,\n",
       "        0.00919813, 0.00073447, 0.01645157, 0.02956688, 0.02557708,\n",
       "        0.01463628, 0.03136475]),\n",
       " 'mean_score_time': array([0.00392566, 0.00318451, 0.00359154, 0.00332551, 0.00356846,\n",
       "        0.00187607, 0.0015295 , 0.00428171, 0.00227289, 0.00210066,\n",
       "        0.00440798, 0.00317221]),\n",
       " 'std_score_time': array([2.98505661e-03, 4.74272996e-04, 4.12560092e-03, 2.26901014e-03,\n",
       "        1.78431199e-03, 4.66750059e-04, 1.11108174e-05, 2.02040159e-03,\n",
       "        9.73634718e-04, 7.03284575e-04, 3.63571996e-03, 1.67252581e-03]),\n",
       " 'param_alpha': masked_array(data=[0.001, 0.01, 0.1, 1, 2, 10, 50, 100, 200, 300, 500,\n",
       "                    1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'alpha': 0.001},\n",
       "  {'alpha': 0.01},\n",
       "  {'alpha': 0.1},\n",
       "  {'alpha': 1},\n",
       "  {'alpha': 2},\n",
       "  {'alpha': 10},\n",
       "  {'alpha': 50},\n",
       "  {'alpha': 100},\n",
       "  {'alpha': 200},\n",
       "  {'alpha': 300},\n",
       "  {'alpha': 500},\n",
       "  {'alpha': 1000}],\n",
       " 'split0_test_score': array([0.03695737, 0.03698276, 0.03723619, 0.039719  , 0.04237205,\n",
       "        0.06023771, 0.1034665 , 0.12047742, 0.12764015, 0.12657165,\n",
       "        0.12035387, 0.1049627 ]),\n",
       " 'split1_test_score': array([0.00834662, 0.00836867, 0.00858879, 0.01075173, 0.0130765 ,\n",
       "        0.02916634, 0.07388812, 0.09778627, 0.11585165, 0.12138644,\n",
       "        0.12193122, 0.11113725]),\n",
       " 'split2_test_score': array([-0.01776472, -0.01772721, -0.01735274, -0.01366737, -0.00969487,\n",
       "         0.01811207,  0.09776074,  0.14072702,  0.17130318,  0.17853857,\n",
       "         0.17486849,  0.15045608]),\n",
       " 'split3_test_score': array([0.05464836, 0.05466953, 0.05488076, 0.05695108, 0.05916508,\n",
       "        0.07410913, 0.10948776, 0.12007143, 0.11574984, 0.10499529,\n",
       "        0.08475314, 0.05269808]),\n",
       " 'split4_test_score': array([-0.11222842, -0.11218948, -0.11180068, -0.10797838, -0.10386684,\n",
       "        -0.0753635 ,  0.00271797,  0.04225342,  0.07095651,  0.08073488,\n",
       "         0.08628538,  0.08410605]),\n",
       " 'mean_test_score': array([-0.00600816, -0.00597914, -0.00568954, -0.00284479,  0.00021038,\n",
       "         0.02125235,  0.07746422,  0.10426311,  0.12030026,  0.12244537,\n",
       "         0.11763842,  0.10067203]),\n",
       " 'std_test_score': array([0.05857008, 0.05856364, 0.05849932, 0.05786586, 0.05718213,\n",
       "        0.05238476, 0.03927687, 0.03385136, 0.03203271, 0.03227651,\n",
       "        0.03275832, 0.03219414]),\n",
       " 'rank_test_score': array([12, 11, 10,  9,  8,  7,  6,  4,  2,  1,  3,  5], dtype=int32)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearchcv.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2072ab14",
   "metadata": {},
   "source": [
    "As you can see it is a dictionary with quite a lot of stuff, and somewhat difficult to read. However, it can be easily converted into a pandas dataframe for easier inspection: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d6464f54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142650</td>\n",
       "      <td>0.037989</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.002985</td>\n",
       "      <td>0.001</td>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>0.036957</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>-0.017765</td>\n",
       "      <td>0.054648</td>\n",
       "      <td>-0.112228</td>\n",
       "      <td>-0.006008</td>\n",
       "      <td>0.058570</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148492</td>\n",
       "      <td>0.038581</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>0.036983</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>-0.017727</td>\n",
       "      <td>0.054670</td>\n",
       "      <td>-0.112189</td>\n",
       "      <td>-0.005979</td>\n",
       "      <td>0.058564</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140513</td>\n",
       "      <td>0.040717</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>0.037236</td>\n",
       "      <td>0.008589</td>\n",
       "      <td>-0.017353</td>\n",
       "      <td>0.054881</td>\n",
       "      <td>-0.111801</td>\n",
       "      <td>-0.005690</td>\n",
       "      <td>0.058499</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.155356</td>\n",
       "      <td>0.025284</td>\n",
       "      <td>0.003326</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>1</td>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>0.039719</td>\n",
       "      <td>0.010752</td>\n",
       "      <td>-0.013667</td>\n",
       "      <td>0.056951</td>\n",
       "      <td>-0.107978</td>\n",
       "      <td>-0.002845</td>\n",
       "      <td>0.057866</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.160210</td>\n",
       "      <td>0.036280</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>2</td>\n",
       "      <td>{'alpha': 2}</td>\n",
       "      <td>0.042372</td>\n",
       "      <td>0.013076</td>\n",
       "      <td>-0.009695</td>\n",
       "      <td>0.059165</td>\n",
       "      <td>-0.103867</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.057182</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.091165</td>\n",
       "      <td>0.009198</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>10</td>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>0.060238</td>\n",
       "      <td>0.029166</td>\n",
       "      <td>0.018112</td>\n",
       "      <td>0.074109</td>\n",
       "      <td>-0.075364</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>0.052385</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.108567</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>50</td>\n",
       "      <td>{'alpha': 50}</td>\n",
       "      <td>0.103466</td>\n",
       "      <td>0.073888</td>\n",
       "      <td>0.097761</td>\n",
       "      <td>0.109488</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.077464</td>\n",
       "      <td>0.039277</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.143065</td>\n",
       "      <td>0.016452</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>100</td>\n",
       "      <td>{'alpha': 100}</td>\n",
       "      <td>0.120477</td>\n",
       "      <td>0.097786</td>\n",
       "      <td>0.140727</td>\n",
       "      <td>0.120071</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>0.104263</td>\n",
       "      <td>0.033851</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.169469</td>\n",
       "      <td>0.029567</td>\n",
       "      <td>0.002273</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>200</td>\n",
       "      <td>{'alpha': 200}</td>\n",
       "      <td>0.127640</td>\n",
       "      <td>0.115852</td>\n",
       "      <td>0.171303</td>\n",
       "      <td>0.115750</td>\n",
       "      <td>0.070957</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.032033</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.131456</td>\n",
       "      <td>0.025577</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>300</td>\n",
       "      <td>{'alpha': 300}</td>\n",
       "      <td>0.126572</td>\n",
       "      <td>0.121386</td>\n",
       "      <td>0.178539</td>\n",
       "      <td>0.104995</td>\n",
       "      <td>0.080735</td>\n",
       "      <td>0.122445</td>\n",
       "      <td>0.032277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.146389</td>\n",
       "      <td>0.014636</td>\n",
       "      <td>0.004408</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>500</td>\n",
       "      <td>{'alpha': 500}</td>\n",
       "      <td>0.120354</td>\n",
       "      <td>0.121931</td>\n",
       "      <td>0.174868</td>\n",
       "      <td>0.084753</td>\n",
       "      <td>0.086285</td>\n",
       "      <td>0.117638</td>\n",
       "      <td>0.032758</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.139951</td>\n",
       "      <td>0.031365</td>\n",
       "      <td>0.003172</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'alpha': 1000}</td>\n",
       "      <td>0.104963</td>\n",
       "      <td>0.111137</td>\n",
       "      <td>0.150456</td>\n",
       "      <td>0.052698</td>\n",
       "      <td>0.084106</td>\n",
       "      <td>0.100672</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_alpha  \\\n",
       "0        0.142650      0.037989         0.003926        0.002985       0.001   \n",
       "1        0.148492      0.038581         0.003185        0.000474        0.01   \n",
       "2        0.140513      0.040717         0.003592        0.004126         0.1   \n",
       "3        0.155356      0.025284         0.003326        0.002269           1   \n",
       "4        0.160210      0.036280         0.003568        0.001784           2   \n",
       "5        0.091165      0.009198         0.001876        0.000467          10   \n",
       "6        0.108567      0.000734         0.001530        0.000011          50   \n",
       "7        0.143065      0.016452         0.004282        0.002020         100   \n",
       "8        0.169469      0.029567         0.002273        0.000974         200   \n",
       "9        0.131456      0.025577         0.002101        0.000703         300   \n",
       "10       0.146389      0.014636         0.004408        0.003636         500   \n",
       "11       0.139951      0.031365         0.003172        0.001673        1000   \n",
       "\n",
       "              params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0   {'alpha': 0.001}           0.036957           0.008347          -0.017765   \n",
       "1    {'alpha': 0.01}           0.036983           0.008369          -0.017727   \n",
       "2     {'alpha': 0.1}           0.037236           0.008589          -0.017353   \n",
       "3       {'alpha': 1}           0.039719           0.010752          -0.013667   \n",
       "4       {'alpha': 2}           0.042372           0.013076          -0.009695   \n",
       "5      {'alpha': 10}           0.060238           0.029166           0.018112   \n",
       "6      {'alpha': 50}           0.103466           0.073888           0.097761   \n",
       "7     {'alpha': 100}           0.120477           0.097786           0.140727   \n",
       "8     {'alpha': 200}           0.127640           0.115852           0.171303   \n",
       "9     {'alpha': 300}           0.126572           0.121386           0.178539   \n",
       "10    {'alpha': 500}           0.120354           0.121931           0.174868   \n",
       "11   {'alpha': 1000}           0.104963           0.111137           0.150456   \n",
       "\n",
       "    split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0            0.054648          -0.112228        -0.006008        0.058570   \n",
       "1            0.054670          -0.112189        -0.005979        0.058564   \n",
       "2            0.054881          -0.111801        -0.005690        0.058499   \n",
       "3            0.056951          -0.107978        -0.002845        0.057866   \n",
       "4            0.059165          -0.103867         0.000210        0.057182   \n",
       "5            0.074109          -0.075364         0.021252        0.052385   \n",
       "6            0.109488           0.002718         0.077464        0.039277   \n",
       "7            0.120071           0.042253         0.104263        0.033851   \n",
       "8            0.115750           0.070957         0.120300        0.032033   \n",
       "9            0.104995           0.080735         0.122445        0.032277   \n",
       "10           0.084753           0.086285         0.117638        0.032758   \n",
       "11           0.052698           0.084106         0.100672        0.032194   \n",
       "\n",
       "    rank_test_score  \n",
       "0                12  \n",
       "1                11  \n",
       "2                10  \n",
       "3                 9  \n",
       "4                 8  \n",
       "5                 7  \n",
       "6                 6  \n",
       "7                 4  \n",
       "8                 2  \n",
       "9                 1  \n",
       "10                3  \n",
       "11                5  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results = pd.DataFrame(gridsearchcv.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342e342",
   "metadata": {},
   "source": [
    "We can see results for each of our 12 model candidates (remember, we used 12 alpha values to define our grid). That is, each row represents the results for 1 model candidate (for which you can see the parameters in the **\"params\"** column). Perhaps most interesting are the **\"mean_test_score\"** and **\"std_test_score\"**, which show us mean accuracy and the standard deviation across the different train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6133a81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'alpha': 0.001}</td>\n",
       "      <td>-0.006008</td>\n",
       "      <td>0.058570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'alpha': 0.01}</td>\n",
       "      <td>-0.005979</td>\n",
       "      <td>0.058564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'alpha': 0.1}</td>\n",
       "      <td>-0.005690</td>\n",
       "      <td>0.058499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'alpha': 1}</td>\n",
       "      <td>-0.002845</td>\n",
       "      <td>0.057866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'alpha': 2}</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.057182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'alpha': 10}</td>\n",
       "      <td>0.021252</td>\n",
       "      <td>0.052385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'alpha': 50}</td>\n",
       "      <td>0.077464</td>\n",
       "      <td>0.039277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'alpha': 100}</td>\n",
       "      <td>0.104263</td>\n",
       "      <td>0.033851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'alpha': 200}</td>\n",
       "      <td>0.120300</td>\n",
       "      <td>0.032033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>{'alpha': 300}</td>\n",
       "      <td>0.122445</td>\n",
       "      <td>0.032277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>{'alpha': 500}</td>\n",
       "      <td>0.117638</td>\n",
       "      <td>0.032758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>{'alpha': 1000}</td>\n",
       "      <td>0.100672</td>\n",
       "      <td>0.032194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              params  mean_test_score  std_test_score\n",
       "0   {'alpha': 0.001}        -0.006008        0.058570\n",
       "1    {'alpha': 0.01}        -0.005979        0.058564\n",
       "2     {'alpha': 0.1}        -0.005690        0.058499\n",
       "3       {'alpha': 1}        -0.002845        0.057866\n",
       "4       {'alpha': 2}         0.000210        0.057182\n",
       "5      {'alpha': 10}         0.021252        0.052385\n",
       "6      {'alpha': 50}         0.077464        0.039277\n",
       "7     {'alpha': 100}         0.104263        0.033851\n",
       "8     {'alpha': 200}         0.120300        0.032033\n",
       "9     {'alpha': 300}         0.122445        0.032277\n",
       "10    {'alpha': 500}         0.117638        0.032758\n",
       "11   {'alpha': 1000}         0.100672        0.032194"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[[\"params\", \"mean_test_score\", \"std_test_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e14ebc5",
   "metadata": {},
   "source": [
    "We could quickly plot the scoring metric against the alpha parameter to get a quick, more intuitive overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be7d801c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='param_alpha'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv_results.plot.line(x=\"param_alpha\", y=[\"mean_test_score\", \"std_test_score\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8b3913",
   "metadata": {},
   "source": [
    "Although it may be difficult to see the exact parameter at which **r-squared** is greatest, we can roughly see that a higher alpha increases performance up to an alpha between 200 and 300 and then decreases again (we know from the exact values in the table that alpha=300 gives use the best score, but in a larger, more fine-grained grid this may not be as easy to see).\n",
    "\n",
    "Conveniently, since the GridSearchCV had the **\"refit\"** parameter set to True (as default), it already selects the best scoring model and refits it on all of the data we gave it, so that we can now use it directly for further testing. We can check the best model and its parameters as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "04e3a312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Ridge(alpha=300)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Ridge</label><div class=\"sk-toggleable__content\"><pre>Ridge(alpha=300)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "Ridge(alpha=300)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearchcv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6617fcc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 300}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearchcv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e025cd75",
   "metadata": {},
   "source": [
    "As expected, the model is a Ridge Regressor, but we can also see that the alpha value fitted for the best model corresponds to the alpha parameter for which we can see the highest score in the **cv_results** table. Let us now try to evaluate this best model on the final holdout set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4a839d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_predictions_gscv = gridsearchcv.predict(X_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81f9cb6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12744537475411344"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y_holdout, holdout_predictions_gscv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e698c0",
   "metadata": {},
   "source": [
    "Let us also quickly plot the predicted values versus the actual observed values in a scatter plot to see what our data looks like. Let us for this import seaborns regplot, which lends itself quite nicely to these types of plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6836816e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: xlabel='predicted', ylabel='observed'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "predictions_vs_observed = pd.DataFrame(\n",
    "    {\n",
    "        \"predicted\": holdout_predictions_gscv,\n",
    "        \"observed\": y_holdout\n",
    "    }\n",
    ")\n",
    "sns.regplot(data=predictions_vs_observed, x=\"predicted\", y=\"observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e9dd0e",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Do a similar grid search for other model families of your choice. Can you search grids with multiple hyperparameters (rather than just the one alpha parameter that we used in the example)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa294d9",
   "metadata": {},
   "source": [
    "# Additional Reading or Resources\n",
    "\n",
    "* [The scikit-learn user guide has some good explanations on these topics in its user guide. For example you can look at computing cross-validated metrics](https://scikit-learn.org/stable/modules/cross_validation.html#computing-cross-validated-metrics)\n",
    "* [Tuning hyper-parameters using grid search](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "* [This article talks a bit more about train-test split evaluation](https://machinelearningmastery.com/train-test-split-for-evaluating-machine-learning-algorithms/)\n",
    "* The scikit-learn user guide has some explanations and demonstrations of the above mentioned models [here](https://scikit-learn.org/stable/supervised_learning.html) and it usually links to some relevant papers or books as well.\n",
    "It is well worth trying to read and understand as much as possible about the individual algorithms you are planning to use in your research.\n",
    "* [This is a nice video on ridge regression](https://www.youtube.com/watch?v=Q81RR3yKn30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5741ed",
   "metadata": {},
   "source": [
    "# Sklearn's pipeline\n",
    "\n",
    "Since we often want to apply preprocessing steps before fitting our models, and we want to do so in a cross-validation consistent way, so that we avoid data leakage and over-optimistic accuracy estimates. We need an easy way to chain different steps. The scikit-learn **pipeline** object provides just that.\n",
    "\n",
    "Check out its documentation below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0424d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "?Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cb6928",
   "metadata": {},
   "source": [
    "The main parameter we care about is the **\"steps\"** parameter. As the name (and the description) suggests, it is a list of the steps that we want to apply in our pipeline. Each step consists of a tuple, that first indicates the name of the step, and then hands over the actual object associated with that step.\n",
    "\n",
    "Let's imagine for example, that we want to make a pipeline in which we first perform a **principal component analysis (PCA)** to extract components, and then fit a **Support Vector Regression (SVR)** on only those extracted components. This may be quite complicated to implement in code especially if we also want to do some hyperparameter tuning in a cross-validated grid search, but combining the two steps in a pipeline makes it much more convenient.\n",
    "\n",
    "First let's prepare the PCA and the SVR and put them into a list of steps for the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e4cdcd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "pca = PCA()\n",
    "svr = SVR()\n",
    "\n",
    "steps = [(\"pca\", pca), (\"svr\", svr)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b683f0",
   "metadata": {},
   "source": [
    "Let's also check out the hyperparameters for the SVR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "adb19cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "?SVR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c8bcd0",
   "metadata": {},
   "source": [
    "There are a number of different kernels, degrees (for the polynomial kernel), gamma, and C values (you can see already, hyperparameter tuning can get increasingly difficult and complex the more models you start considering). Let us try an SVR with a **\"rbf\"** kernel and some different values for gamma and C. We also want to set a number of components to extract with the PCA. The way we define a grid in a pipeline is to **name the keys of the dictionary** using the **name of the step** and the **name of the parameter** separated by a **double underscore**:\n",
    "\n",
    "**nameofstep__nameofparameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f00012c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_svr_grid = {\n",
    "    \"pca__n_components\": [5, 50, 500],\n",
    "    \"svr__kernel\": [\"rbf\"],\n",
    "    \"svr__C\": np.linspace(0.0001, 1000, 5),\n",
    "    \"svr__gamma\": np.linspace(0.001, 1000, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8a8d6",
   "metadata": {},
   "source": [
    "We also need to define an additional grid to perform the same pipeline without any PCA, as this will be an important setting to try as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bc7a14fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pca_svr_grid = {\n",
    "    \"pca\": [\"passthrough\"],\n",
    "    \"svr__kernel\": [\"rbf\"],\n",
    "    \"svr__C\": np.linspace(0.0001, 1000, 5),\n",
    "    \"svr__gamma\": np.linspace(0.001, 1000, 5)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726d359e",
   "metadata": {},
   "source": [
    "Check out the example below and you can see it is really quite simple (Running it will take quite some time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea2a571",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "pipeline = Pipeline(steps=steps)\n",
    "gridsearch_pipeline = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=[pca_svr_grid, no_pca_svr_grid],\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=100),\n",
    "    scoring=\"r2\",\n",
    "    n_jobs=-1, # for speed up run on all cores\n",
    ")\n",
    "gridsearch_pipeline.fit(X_model_selection, y_model_selection)\n",
    "cv_results = pd.DataFrame(gridsearch_pipeline.cv_results_)\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f66760",
   "metadata": {},
   "source": [
    "This results in quite a big table. We can quickly find the best row using the pandas **.query()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e092335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results.query(\"rank_test_score == 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9a1b09",
   "metadata": {},
   "source": [
    "We can also think about visualising our hyperparameters using some heatmaps (we can make one heatmap for each n_components setting since we can plot the parameters gamma and C on a 2D grid):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24898c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# rename the rows where no pca was performed\n",
    "cv_results[\"param_pca__n_components\"] = cv_results[\"param_pca__n_components\"].replace(\n",
    "    {np.nan: \"no pca performed\"}\n",
    ")\n",
    "\n",
    "# prepare a figure\n",
    "fig, axs = plt.subplots(ncols=1, nrows=4, figsize=(15, 25))\n",
    "\n",
    "# Loop over every PCA parameter to make one subplot for each PCA setting\n",
    "for i, pca_n_comps in enumerate(cv_results[\"param_pca__n_components\"].unique()):\n",
    "    # subset the data for this specific pca setting\n",
    "    mask = cv_results[\"param_pca__n_components\"] == pca_n_comps\n",
    "    subsampled_results = cv_results.loc[mask][[\"param_svr__C\", \"param_svr__gamma\", \"mean_test_score\"]]\n",
    "    \n",
    "    # we do this only because we want to annoying pandas warning about inferring\n",
    "    # numeric data type, we don't care about this being numeric in the plot, as\n",
    "    # it represents our xtick and ytick labels\n",
    "    indices = [\"param_svr__C\", \"param_svr__gamma\"]\n",
    "    subsampled_results[indices] = subsampled_results[indices].astype(str)\n",
    "    \n",
    "    # this will reshape the data from a \"long\" to a \"wide\" format so we can represent\n",
    "    # it in a heatmap with one parameter on the x axis and one parameter on the y axis\n",
    "    heatmap_array = subsampled_results.pivot(\n",
    "        index=\"param_svr__C\", columns=\"param_svr__gamma\", values=\"mean_test_score\"\n",
    "    ).round(decimals=4)\n",
    "    \n",
    "    # this actually makes the subplot\n",
    "    axs[i].set_title(f\"pca components used: {pca_n_comps}\")\n",
    "    sns.heatmap(\n",
    "        heatmap_array,\n",
    "        ax=axs[i],\n",
    "        cmap=\"coolwarm\",\n",
    "        vmin=cv_results[\"mean_test_score\"].min(),\n",
    "        vmax=cv_results[\"mean_test_score\"].max(),\n",
    "        annot=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d76b0d",
   "metadata": {},
   "source": [
    "Over time you may find that these types of visualisations can be quite helpful when searching over some hyperparameter grids. For example, here we can quickly see that performance is best when no PCA is used or when extracting a lot of components rather than only a few.\n",
    "\n",
    "These kinds of visualisations can also help identify problems with the grid. For example if we find that best performing hyperparameters tend to be towards the boundaries we may want to expand our search grid to include more candidate values. For example in this particular instance we may want to include larger C values to see if we hit a plateau or whether scores will further increase or decrease.\n",
    "\n",
    "**Importantly**, however, keep in mind that these scores above **are not** final evaluation scores. They merely serve to compare different model candidates, but will be over-optimistic as evaluation scores. **We have to still take the best model and evaluate it on some data that so far the model has not seen.** Let's do this using the GridSearchCV object, which again, has refit the best model on all the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82218e2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gridsearch_pipeline.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2ac98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "holdout_predictions = gridsearch_pipeline.predict(X_holdout)\n",
    "pipeline_score = r2_score(y_holdout, holdout_predictions)\n",
    "pipeline_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad334bed",
   "metadata": {},
   "source": [
    "Don't be surprised if the score has gone down a bit: This is precisely what is meant when we say that the estimated score during model selection is over-optimistic. If the score drops by a lot, however, this is a good sign that we overfit the model during model selection. Let's quickly plot those predictions again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf8b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_vs_observed = pd.DataFrame(\n",
    "    {\n",
    "        \"predicted\": holdout_predictions,\n",
    "        \"observed\": y_holdout\n",
    "    }\n",
    ")\n",
    "sns.regplot(data=predictions_vs_observed, x=\"predicted\", y=\"observed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdd881c",
   "metadata": {},
   "source": [
    "# Exercise:\n",
    "\n",
    "1. One very popular method of preprocessing where data leakage can happen quite easily is feature selection. Take one of the feature selection methods [in-built in scikit-learn](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection) and add it to a pipeline with a regressor of your choice. Can you do hyperparameter tuning for the feature selection process and the estimator simultaneously?\n",
    "Hint: check out the scikit-learn user guide to see [how you can use an F-test to select relevant features here](https://scikit-learn.org/stable/modules/feature_selection.html#univariate-feature-selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26ada44",
   "metadata": {},
   "source": [
    "# Bonus Material: How to perform a cross-validated grid search for both **model family** and **hyperparameters** simultaneously?\n",
    "\n",
    "Since we usually want to select the best model for a given problem from a set of **different model families** and their **associated hyperparameters**, you may now wonder how to do this with the GridSearchCV. We can see easily how it is done with one estimator, but it is not easy to see how to do this with a set of different estimators. This is another use case where the scikit-learn pipeline object can come in quite handy!\n",
    "\n",
    "We can initialise a pipeline with an estimator as a step, and then replace **this estimator** in the **pipeline** with **other type of estimators** using different parameter grids in our **GridSearchCV**, very similar to the way in which we tested the pipeline with and without PCA in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c08269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only define one step, we care only about the regressor and its hyperparameters.\n",
    "# We arbitrarily initialise the pipeline with ridge:\n",
    "pipeline = Pipeline(steps=[(\"regressor\", Ridge())])\n",
    "\n",
    "# parameters for the ridge regressor\n",
    "ridge_params = {\n",
    "    \"regressor\": [Ridge()], # parameters have to be handed over as iterables!\n",
    "    \"regressor__alpha\": np.linspace(0.0001, 1000, 10),\n",
    "}\n",
    "\n",
    "# parameters for the support vector regressor:\n",
    "svr_params = {\n",
    "    \"regressor\": [SVR()],\n",
    "    \"regressor__C\": np.linspace(0.0001, 1000, 10),\n",
    "    \"regressor__kernel\": [\"linear\", \"rbf\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ed6b2",
   "metadata": {},
   "source": [
    "After defining the estimators, the pipeline, and the parameter grids we can put it all together as follows and run the search. This may take a few minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dba3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=[ridge_params, svr_params],\n",
    "    scoring=\"r2\",\n",
    "    cv=kfold,\n",
    "    n_jobs=-1 # to speed up computation, '-1' means it will use all available CPU's\n",
    ")\n",
    "gridsearch_cv.fit(X_model_selection, y_model_selection)\n",
    "pd.DataFrame(gridsearch_cv.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5ec0e",
   "metadata": {},
   "source": [
    "Let's evaluate again the best model on the holdout data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ae71be",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f72876",
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv_predictions = gridsearch_cv.predict(X_holdout)\n",
    "r2_score(y_holdout, gscv_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7f493",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_vs_observed = pd.DataFrame(\n",
    "    {\n",
    "        \"predicted\": gscv_predictions,\n",
    "        \"observed\": y_holdout\n",
    "    }\n",
    ")\n",
    "sns.regplot(data=predictions_vs_observed, x=\"predicted\", y=\"observed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "fc3b3c115f4411572977c7ef81a986974e91e5e03325ec386353451c9b2fc755"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
